{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_path)\n",
    "model['hyper_parameters']['autoencoder_checkpoint'] = '/home/john/PhD/BISCUIT/pretrained_models/AE_gridworld_simplified/AE_40l_64hid.ckpt'\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/miniconda3/envs/biscuit/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m autoencoder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/john/PhD/BISCUIT/pretrained_moels/AE_gridworld_simplified/AE_20l_64hid.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/john/PhD/BISCUIT/pretrained_models/AE_gridworld_simplified/NF_text_only_cardinal.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBISCUITNF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoencoder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoencoder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mfreeze()\n",
      "File \u001b[0;32m~/miniconda3/envs/biscuit/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1543\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1465\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1470\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1471\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;124;03m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;124;03m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;124;03m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1543\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/miniconda3/envs/biscuit/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:63\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_from_checkpoint\u001b[39m(\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Union[Type[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m], Type[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningDataModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[1;32m     56\u001b[0m     checkpoint_path: Union[_PATH, IO],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     61\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningDataModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m---> 63\u001b[0m         checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[1;32m     67\u001b[0m         checkpoint, checkpoint_path\u001b[38;5;241m=\u001b[39m(checkpoint_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint_path, (\u001b[38;5;28mstr\u001b[39m, Path)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     68\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/biscuit/lib/python3.11/site-packages/lightning_fabric/utilities/cloud_io.py:52\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     50\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(path_or_url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/biscuit/lib/python3.11/site-packages/torch/serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/biscuit/lib/python3.11/site-packages/torch/serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1443\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/biscuit/lib/python3.11/pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1213\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/miniconda3/envs/biscuit/lib/python3.11/pickle.py:1254\u001b[0m, in \u001b[0;36m_Unpickler.load_binpersid\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_binpersid\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1253\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m-> 1254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/biscuit/lib/python3.11/site-packages/torch/serialization.py:1408\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1407\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1408\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/envs/biscuit/lib/python3.11/site-packages/torch/serialization.py:1382\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1382\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1383\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1384\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1387\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/envs/biscuit/lib/python3.11/site-packages/torch/serialization.py:391\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 391\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/biscuit/lib/python3.11/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(obj\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(location))\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/biscuit/lib/python3.11/site-packages/torch/_utils.py:108\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 108\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_sparse\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/biscuit/lib/python3.11/site-packages/torch/cuda/__init__.py:374\u001b[0m, in \u001b[0;36mdevice.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_idx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exchange_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/biscuit/lib/python3.11/site-packages/torch/cuda/__init__.py:302\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    301\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 302\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    306\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "from models.biscuit_nf import BISCUITNF\n",
    "import torch\n",
    "from typing import List, Tuple, Dict, Any, Union, Optional\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "autoencoder_path = '/home/john/PhD/BISCUIT/pretrained_moels/AE_gridworld_simplified/AE_20l_64hid.ckpt'\n",
    "model_path = '/home/john/PhD/BISCUIT/pretrained_models/AE_gridworld_simplified/NF_text_only_cardinal.ckpt'\n",
    "model = BISCUITNF.load_from_checkpoint(model_path, autoencoder_path=autoencoder_path)\n",
    "model.to(device)\n",
    "model.freeze()\n",
    "_ = model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "encs_path = '/home/john/PhD/BISCUIT/pretrained_models/AE_gridworld_simplified/encodings/gridworld_simplified_5c_test_indep.pt'\n",
    "encs_path_drop_last_frame = '/home/john/PhD/BISCUIT/pretrained_models/AE_gridworld_simplified/encodings/gridworld_simplified_5c_drop_last_frame_test_indep.pt'\n",
    "t = torch.load(encs_path)\n",
    "t1 = torch.load(encs_path_drop_last_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "for i in range(50):\n",
    "    print(i)\n",
    "    img1 = t1[i]\n",
    "    img2 = t[i]\n",
    "    print(img1)\n",
    "    print(img2)\n",
    "    img1 = model.autoencoder.decoder(torch.from_numpy(img1).to(device).unsqueeze(0))\n",
    "    img1 = (img1 + 1) / 2\n",
    "    img2 = model.autoencoder.decoder(torch.from_numpy(img2).to(device).unsqueeze(0))\n",
    "    img2 = (img2 + 1) / 2\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(img1.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "    ax[1].imshow(img2.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_path = '/home/gkounto/BISCUIT/experiments/pretrained_models/AE_gridworld_small/encodings/gridworld_small_pre_intv_freeze_test.pt'\n",
    "encodings = torch.load(encodings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.datasets import GridworldDataset\n",
    "data_folder = '/home/john/PhD/BISCUIT/data/gridworld_simplified_5c'\n",
    "# val_seq_dataset = GridworldDataset(\n",
    "# data_folder=data_folder, split='val', return_targets=True, return_robot_state=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=0.01)\n",
    "train_seq_dataset = GridworldDataset(\n",
    "        data_folder=data_folder, split='train', return_targets=True, return_latents=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=True, subsample_percentage=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from data_generation.gridworld import Gridworld\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def format_causal_dict(causal_dict):\n",
    "    formatted_text = \"\"\n",
    "    for key, value in causal_dict.items():\n",
    "        formatted_text += f\"{key}: {value}\\n\"\n",
    "        if \"position_y\" in key:  # Add a line break after each entity's details\n",
    "            formatted_text += \"\\n\"\n",
    "    return formatted_text\n",
    "\n",
    "def visualize_episode(frames, causals, causal_keys, actions, action_descriptions, interventions):\n",
    "    for i in range(len(frames)):\n",
    "        # debug_causals = Gridworld.causal_vector_to_debug_dict(causal_keys, causals[i])\n",
    "        debug_causals = dict(zip(causal_keys, causals[i]))\n",
    "        formatted_causals = format_causal_dict(debug_causals)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax1 = plt.subplot(1, 2, 1)  # Frame subplot\n",
    "        ax1.imshow(frames[i])\n",
    "        ax1.set_title(f\"Frame {i+1}\")\n",
    "        ax1.axis('off')\n",
    "\n",
    "        ax2 = plt.subplot(1, 2, 2)  # Textual information subplot\n",
    "        ax2.axis('off')\n",
    "        text_info = (\n",
    "            f\"Step {i+1}:\\n\\n\"\n",
    "            f\"Causals (Formatted):\\n{formatted_causals}\\n\"\n",
    "            f\"Action: {actions[i]}\\n\"\n",
    "            f\"Action Description: {action_descriptions[i]}\\n\"\n",
    "            f\"Interventions: {interventions[i]}\"\n",
    "        )\n",
    "        ax2.text(0, 1, text_info, ha='left', va='top', fontsize=8, wrap=True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "a = np.load('/home/john/PhD/BISCUIT/data_generation/data/gridworld_simplified_2c2b2l_noturn_noshufflecars/check/gridworld_episode_28.npz')\n",
    "causal_keys = json.load(open('/home/john/PhD/BISCUIT/data_generation/data/gridworld_simplified_2c2b2l_noturn_noshufflecars/check_metadata.json'))['flattened_causals']\n",
    "frames, causals, actions, interventions, action_descriptions = a['frames'], a['causals'], a['actions'], a['interventions'], a['action_descriptions']\n",
    "visualize_episode(frames, causals, causal_keys, actions, action_descriptions, interventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "# Assume Gridworld, format_causal_dict, and the dataset class are defined elsewhere\n",
    "\n",
    "def visualize_episodes(dataset, causal_keys, N=1):\n",
    "    for episode_idx in range(min(N, len(dataset))):  # Loop through N episodes or the total dataset length\n",
    "        frame_seq = dataset[episode_idx]  # Get the episode data\n",
    "        frames, actions, interventions, causals = frame_seq\n",
    "        \n",
    "        for i in range(frames.shape[0] - 1):  # Iterate through each step, excluding the last frame\n",
    "            # Map causals to keys for the current frame\n",
    "            debug_causals = dict(zip(causal_keys, causals[i]))\n",
    "            formatted_causals = format_causal_dict(debug_causals)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax1 = plt.subplot(1, 2, 1)  # Frame subplot\n",
    "            # Adjust image normalization if necessary\n",
    "            img = (frames[i].permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0  # Normalize if required\n",
    "            ax1.imshow(img)\n",
    "            ax1.set_title(f\"Episode {episode_idx + 1}, Frame {i + 1}\")\n",
    "            ax1.axis('off')\n",
    "\n",
    "            ax2 = plt.subplot(1, 2, 2)  # Textual information subplot\n",
    "            ax2.axis('off')\n",
    "            text_info = (\n",
    "                f\"Episode {episode_idx + 1}, Step {i + 1}:\\n\\n\"\n",
    "                f\"Causals (Formatted):\\n{formatted_causals}\\n\"\n",
    "                f\"Action: {actions[i]}\\n\"\n",
    "                f\"Interventions: {interventions[i]}\"\n",
    "            )\n",
    "            ax2.text(0, 1, text_info, ha='left', va='top', fontsize=8, wrap=True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Example usage:\n",
    "causal_keys = json.load(open('/home/gkounto/BISCUIT/data_generation/data/gridworld_simplified/train_metadata.json'))['flattened_causals']\n",
    "# Assuming train_seq_dataset is defined and loaded\n",
    "visualize_episodes(train_seq_dataset, causal_keys, N=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_img(imgs: Any,\n",
    "             figure_title: str = None,\n",
    "             titles: Optional[list] = None):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axes = plt.subplots(1, len(imgs), figsize=(len(imgs) * 3.5, 3.5))\n",
    "    if len(imgs) == 1:\n",
    "        axes = [axes]\n",
    "    for i, ax in enumerate(axes):\n",
    "        if len(imgs[i].shape) == 3 and imgs[i].shape[0] in [3,4]:\n",
    "            imgs[i] = imgs[i].permute(1, 2, 0)\n",
    "        if isinstance(imgs[i], torch.Tensor):\n",
    "            imgs[i] = imgs[i].detach().cpu().numpy()\n",
    "        ax.imshow(imgs[i])\n",
    "        ax.axis('off')\n",
    "        if titles is not None and i < len(titles):\n",
    "            ax.set_title(titles[i], weight='bold')\n",
    "    if figure_title is not None:\n",
    "        fig.suptitle(figure_title, weight='bold', size=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_exclamation_mark_image(size=256, background_color=(1, 1, 1), mark_color=(1, 0, 0), mark_thickness=10, mark_height=100, dot_radius=10):\n",
    "    \"\"\"\n",
    "    Creates an image with a red exclamation mark in the center on a white background.\n",
    "\n",
    "    Parameters:\n",
    "    - size (int): The size of the image (width and height). Default is 256.\n",
    "    - background_color (tuple): The RGB color of the background in the range 0 to 1. Default is white.\n",
    "    - mark_color (tuple): The RGB color of the exclamation mark in the range 0 to 1. Default is red.\n",
    "    - mark_thickness (int): The thickness of the line part of the exclamation mark. Default is 10.\n",
    "    - mark_height (int): The height of the line part of the exclamation mark. Default is 100.\n",
    "    - dot_radius (int): The radius of the dot part of the exclamation mark. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The generated image as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Create a size x size x 3 array with the background color\n",
    "    image = np.ones((size, size, 3)) * np.array(background_color)\n",
    "\n",
    "    # Define the center position\n",
    "    center_x, center_y = size // 2, size // 2\n",
    "\n",
    "    # Draw the line part of the exclamation mark\n",
    "    for x in range(center_x - mark_thickness // 2, center_x + mark_thickness // 2):\n",
    "        for y in range(center_y - mark_height // 2, center_y + mark_height // 2 - dot_radius * 2):\n",
    "            image[y, x] = mark_color\n",
    "\n",
    "    # Draw the dot part of the exclamation mark\n",
    "    for x in range(image.shape[1]):\n",
    "        for y in range(image.shape[0]):\n",
    "            if (x - center_x) ** 2 + (y - (center_y + mark_height // 2 + dot_radius)) ** 2 <= dot_radius ** 2:\n",
    "                image[y, x] = mark_color\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def next_step_prediction(\n",
    "        model: BISCUITNF,\n",
    "        image: torch.Tensor,\n",
    "        action: torch.Tensor,\n",
    "        gt_image: torch.Tensor = torch.zeros(1),\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        plot_images: bool = True,\n",
    "        intv_targets: Optional[torch.Tensor] = None,\n",
    "        text: Optional[str] = None,\n",
    "        tokenizer: Optional[Any] = None,\n",
    "        text_only: bool = False,\n",
    "        N: int = 8\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if latents is None:\n",
    "        input_image = (image * 2.0) - 1.0\n",
    "        latents = model.autoencoder.encoder(input_image[None])\n",
    "        latents, _ = model.flow.forward(latents)\n",
    "    if text is not None:\n",
    "        tokenized_description = tokenizer.tokenizer(text, return_token_type_ids=True, padding='max_length', max_length=64)\n",
    "        input_ids = torch.tensor(tokenized_description['input_ids']).to(device)\n",
    "        token_type_ids = torch.tensor(tokenized_description['token_type_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokenized_description['attention_mask']).to(device)\n",
    "        tokenized_description = {'input_ids': input_ids, 'token_type_ids': token_type_ids, 'attention_mask': attention_mask}    \n",
    "    new_latents, _ = model.prior_t1.sample(latents, action[None], num_samples=1, intv_targets=intv_targets, tokenized_description=tokenized_description)\n",
    "    new_latents = new_latents.squeeze(1)\n",
    "    new_encodings = model.flow.reverse(new_latents)\n",
    "    new_image = model.autoencoder.decoder(new_encodings)[0]\n",
    "    new_image = (new_image + 1.0) / 2.0\n",
    "    if plot_images:\n",
    "        gt_diff_flag = False\n",
    "        new_image_frame = new_image.permute(1, 2, 0).cpu().numpy()\n",
    "        if latents is None:\n",
    "            old_image_frame = (image.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0\n",
    "        else:\n",
    "            old_image_frame = image.permute(1, 2, 0).cpu().numpy()\n",
    "        clicked_image_frame = np.copy(old_image_frame)\n",
    "        ground_truth_image_frame = (gt_image.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0\n",
    "        difference = np.abs(new_image_frame - ground_truth_image_frame)\n",
    "        print(f\"Mean absolute difference between the new image and the ground truth: {difference.mean()}\")\n",
    "        if difference.mean() > 0.001:\n",
    "            print(\"The new image is not close to the ground truth\")\n",
    "            gt_diff_flag = True\n",
    "            exclamation_mark_image = create_exclamation_mark_image()\n",
    "        if action.ndim == 1:\n",
    "            action = action[None]\n",
    "        for i in range(action.shape[0]):\n",
    "            if torch.any(action < 0) or torch.any(action > 1):\n",
    "                continue\n",
    "            # Correct calculation for pixel positions considering the normalized action coordinates\n",
    "            pixel_x = int(action[i, 0].item() * (image.shape[-1] - 1))\n",
    "            pixel_y = int(action[i, 1].item() * (image.shape[-2] - 1))\n",
    "            # Highlight the click location with a red color\n",
    "            clicked_image_frame[max(0, pixel_y-5):pixel_y+6, \n",
    "                                max(0, pixel_x-5):pixel_x+6, \n",
    "                                :] = np.array([1.0, 1.0, 1.0])\n",
    "        if torch.any(gt_image != 0):\n",
    "            image_list = [old_image_frame, clicked_image_frame, new_image_frame, ground_truth_image_frame]\n",
    "            image_titles = ['Previous Frame', 'Click Location', 'New Sample', 'Ground Truth']\n",
    "            if gt_diff_flag:\n",
    "                image_list.append(exclamation_mark_image)\n",
    "                image_titles.append('Difference')\n",
    "            show_img(image_list,\n",
    "                     figure_title=f'Performing action {(action if action.ndim == 1 else action[0]).squeeze().cpu().numpy()}',\n",
    "                     titles=image_titles)\n",
    "        else:\n",
    "            show_img([old_image_frame, clicked_image_frame, new_image_frame],\n",
    "                    figure_title=f'Performing action {(action if action.ndim == 1 else action[0]).squeeze().cpu().numpy()}',\n",
    "                    titles=['Previous Frame', 'Click Location', 'New Sample'])\n",
    "    return new_image, new_latents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = GridworldDataset(\n",
    "\tdata_folder=data_folder, split='val_indep', return_targets=False, single_image=True, return_latents=True, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=1.0,)\n",
    "for elem in val_dataset:\n",
    "    print(elem[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.datasets import GridworldDataset\n",
    "data_folder = '/home/john/PhD/BISCUIT/data/gridworld_simplified_5c'\n",
    "# val_seq_dataset = GridworldDataset(\n",
    "# data_folder=data_folder, split='val', return_targets=True, return_robot_state=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=0.01)\n",
    "test_seq_dataset = GridworldDataset(\n",
    "        data_folder=data_folder, split='test', return_targets=True, return_latents=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=0.01)\n",
    "val_indep_dataset = GridworldDataset(\n",
    "         data_folder=data_folder, split='val_indep', return_targets=True, single_image=True, return_latents=True, triplet=False, seq_len=1, cluster=False, return_text=False, subsample_percentage=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def reverseEnum(data: list):\n",
    "    for i in range(len(data)-1, -1, -1):\n",
    "        yield (i, data[i])\n",
    "\n",
    "for i, frame_seq in enumerate(test_seq_dataset):\n",
    "        print(i)\n",
    "        action = torch.tensor(frame_seq[1]).to(device).squeeze()\n",
    "        interventions = torch.tensor(frame_seq[2]).to(device)\n",
    "        causals = torch.tensor(frame_seq[3]).to(device)\n",
    "        print(causals)\n",
    "        print(interventions)\n",
    "        # intv_targets = torch.tensor(frame_seq[2]).to(device)\n",
    "        intv_targets = None\n",
    "        plot_images = True\n",
    "        new_image, new_latents = next_step_prediction(model, frame_seq[0][0], action, gt_image=frame_seq[0][1], plot_images=plot_images, intv_targets=intv_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, frame_seq in enumerate(val_seq_dataset):\n",
    "    # Stack the images at each step to form a sequence\n",
    "    frames, actions = frame_seq\n",
    "    frames = frames.to(device)\n",
    "    actions = torch.from_numpy(actions).to(device)\n",
    "    # interventions = interventions.to(device)\n",
    "    # causals = causals.to(device)\n",
    "    # Perform the next step prediction\n",
    "    new_image, new_latents = next_step_prediction(frames[0], actions, gt_image=frames[1], intv_targets=None, plot_images=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "latents = None\n",
    "image = test_seq_dataset[0][0][0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "load_text = ax.text(image.shape[-1]//2, \n",
    "                image.shape[-2]//2, \n",
    "                'Loading...', \n",
    "                fontsize='x-large',\n",
    "                weight='bold',\n",
    "                va='center',\n",
    "                ha='center',\n",
    "                backgroundcolor=(1.0, 0.8, 0.8))\n",
    "load_text.set_visible(False)\n",
    "ax.axis('off')\n",
    "\n",
    "def onclick(event):\n",
    "    print('click')\n",
    "    global image, latents\n",
    "    load_text.set_visible(True)\n",
    "    fig.canvas.draw()\n",
    "    ix, iy = event.xdata, event.ydata\n",
    "    # ix = (ix / image.shape[-1] - 0.5) * 2.0\n",
    "    # iy = (iy / image.shape[-2] - 0.5) * 2.0\n",
    "    print(f'Clicked at x={ix}, y={iy}')\n",
    "    image, latents = next_step_prediction(model, image=image,\n",
    "                                            action=torch.tensor([iy, ix], \n",
    "                                                                    dtype=torch.float32,\n",
    "                                                                    device=device),\n",
    "                                            latents=latents,\n",
    "                                            plot_images=False)\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    load_text.set_visible(False)\n",
    "    fig.canvas.draw()\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = test_seq_dataset[5][0][0]\n",
    "latents = None\n",
    "action_sequence = [\n",
    "    (0.42857143, 1.),\n",
    "    (0., 0.),\n",
    "    (-0.14285714, -0.14285714),\n",
    "    (0.42857143, 1.),\n",
    "    (0., 0.),\n",
    "]\n",
    "\n",
    "for i, action in enumerate(action_sequence):\n",
    "    image, latents = next_step_prediction(model,\n",
    "                                          image=image, \n",
    "                                          action=torch.tensor(action, device=device), \n",
    "                                          latents=latents,\n",
    "                                          gt_image=test_seq_dataset[5][0][1],\n",
    "                                          plot_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "# Assume Gridworld, format_causal_dict, and the dataset class are defined elsewhere\n",
    "\n",
    "def visualize_episodes(dataset, causal_keys, N=1):\n",
    "    for episode_idx in range(min(N, len(dataset))):  # Loop through N episodes or the total dataset length\n",
    "        frame_seq = dataset[episode_idx]  # Get the episode data\n",
    "        frames, actions, interventions, causals = frame_seq\n",
    "        \n",
    "        for i in range(frames.shape[0] - 1):  # Iterate through each step, excluding the last frame\n",
    "            # Map causals to keys for the current frame\n",
    "            debug_causals = dict(zip(causal_keys, causals[i]))\n",
    "            print(debug_causals)\n",
    "            formatted_causals = format_causal_dict(debug_causals)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax1 = plt.subplot(1, 2, 1)  # Frame subplot\n",
    "            # Adjust image normalization if necessary\n",
    "            img = (frames[i].permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0  # Normalize if required\n",
    "            ax1.imshow(img)\n",
    "            ax1.set_title(f\"Episode {episode_idx + 1}, Frame {i + 1}\")\n",
    "            ax1.axis('off')\n",
    "\n",
    "            ax2 = plt.subplot(1, 2, 2)  # Textual information subplot\n",
    "            ax2.axis('off')\n",
    "            text_info = (\n",
    "                f\"Episode {episode_idx + 1}, Step {i + 1}:\\n\\n\"\n",
    "                f\"Causals (Formatted):\\n{formatted_causals}\\n\"\n",
    "                f\"Action: {actions[i]}\\n\"\n",
    "                f\"Interventions: {interventions[i]}\"\n",
    "            )\n",
    "            ax2.text(0, 1, text_info, ha='left', va='top', fontsize=8, wrap=True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Example usage:\n",
    "causal_keys = json.load(open('/home/john/PhD/BISCUIT/data/gridworld_simplified_5c/train_metadata.json'))['flattened_causals']\n",
    "# Assuming train_seq_dataset is defined and loaded\n",
    "visualize_episodes(train_seq_dataset, causal_keys, N=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_img(imgs: Any,\n",
    "             figure_title: str = None,\n",
    "             titles: Optional[list] = None):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axes = plt.subplots(1, len(imgs), figsize=(len(imgs) * 3.5, 3.5))\n",
    "    if len(imgs) == 1:\n",
    "        axes = [axes]\n",
    "    for i, ax in enumerate(axes):\n",
    "        if len(imgs[i].shape) == 3 and imgs[i].shape[0] in [3,4]:\n",
    "            imgs[i] = imgs[i].permute(1, 2, 0)\n",
    "        if isinstance(imgs[i], torch.Tensor):\n",
    "            imgs[i] = imgs[i].detach().cpu().numpy()\n",
    "        ax.imshow(imgs[i])\n",
    "        ax.axis('off')\n",
    "        if titles is not None and i < len(titles):\n",
    "            ax.set_title(titles[i], weight='bold')\n",
    "    if figure_title is not None:\n",
    "        fig.suptitle(figure_title, weight='bold', size=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_exclamation_mark_image(size=256, background_color=(1, 1, 1), mark_color=(1, 0, 0), mark_thickness=10, mark_height=100, dot_radius=10):\n",
    "    \"\"\"\n",
    "    Creates an image with a red exclamation mark in the center on a white background.\n",
    "\n",
    "    Parameters:\n",
    "    - size (int): The size of the image (width and height). Default is 256.\n",
    "    - background_color (tuple): The RGB color of the background in the range 0 to 1. Default is white.\n",
    "    - mark_color (tuple): The RGB color of the exclamation mark in the range 0 to 1. Default is red.\n",
    "    - mark_thickness (int): The thickness of the line part of the exclamation mark. Default is 10.\n",
    "    - mark_height (int): The height of the line part of the exclamation mark. Default is 100.\n",
    "    - dot_radius (int): The radius of the dot part of the exclamation mark. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The generated image as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Create a size x size x 3 array with the background color\n",
    "    image = np.ones((size, size, 3)) * np.array(background_color)\n",
    "\n",
    "    # Define the center position\n",
    "    center_x, center_y = size // 2, size // 2\n",
    "\n",
    "    # Draw the line part of the exclamation mark\n",
    "    for x in range(center_x - mark_thickness // 2, center_x + mark_thickness // 2):\n",
    "        for y in range(center_y - mark_height // 2, center_y + mark_height // 2 - dot_radius * 2):\n",
    "            image[y, x] = mark_color\n",
    "\n",
    "    # Draw the dot part of the exclamation mark\n",
    "    for x in range(image.shape[1]):\n",
    "        for y in range(image.shape[0]):\n",
    "            if (x - center_x) ** 2 + (y - (center_y + mark_height // 2 + dot_radius)) ** 2 <= dot_radius ** 2:\n",
    "                image[y, x] = mark_color\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def next_step_prediction(\n",
    "        model: BISCUITNF,\n",
    "        image: torch.Tensor,\n",
    "        action: torch.Tensor,\n",
    "        gt_image: torch.Tensor = torch.zeros(1),\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        plot_images: bool = True,\n",
    "        intv_targets: Optional[torch.Tensor] = None,\n",
    "        text: Optional[str] = None,\n",
    "        tokenizer: Optional[Any] = None,\n",
    "        text_only: bool = False,\n",
    "        tokenized_description=None,\n",
    "        N: int = 8\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if latents is None:\n",
    "        input_image = (image * 2.0) - 1.0\n",
    "        latents = model.autoencoder.encoder(input_image[None])\n",
    "        latents, _ = model.flow.forward(latents)\n",
    "    # tokenized_description = None\n",
    "    if text is not None:\n",
    "        tokenized_description = tokenizer(text, return_token_type_ids=True, padding='max_length', max_length=64)\n",
    "        input_ids = torch.tensor(tokenized_description['input_ids']).to(device)\n",
    "        token_type_ids = torch.tensor(tokenized_description['token_type_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokenized_description['attention_mask']).to(device)\n",
    "        tokenized_description = {'input_ids': input_ids, 'token_type_ids': token_type_ids, 'attention_mask': attention_mask}    \n",
    "    new_latents, _ = model.prior_t1.sample(latents, action[None], num_samples=1, intv_targets=intv_targets, tokenized_description=tokenized_description)\n",
    "    new_latents = new_latents.squeeze(1)\n",
    "    new_encodings = model.flow.reverse(new_latents)\n",
    "    new_image = model.autoencoder.decoder(new_encodings)[0]\n",
    "    new_image = (new_image + 1.0) / 2.0\n",
    "    if plot_images:\n",
    "        gt_diff_flag = False\n",
    "        new_image_frame = new_image.permute(1, 2, 0).cpu().numpy()\n",
    "        if latents is None:\n",
    "            old_image_frame = (image.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0\n",
    "        else:\n",
    "            old_image_frame = image.permute(1, 2, 0).cpu().numpy()\n",
    "        clicked_image_frame = np.copy(old_image_frame)\n",
    "        ground_truth_image_frame = (gt_image.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0\n",
    "        difference = np.abs(new_image_frame - ground_truth_image_frame)\n",
    "        print(f\"Mean absolute difference between the new image and the ground truth: {difference.mean()}\")\n",
    "        if difference.mean() > 0.001:\n",
    "            print(\"The new image is not close to the ground truth\")\n",
    "            gt_diff_flag = True\n",
    "            exclamation_mark_image = create_exclamation_mark_image()\n",
    "        if action.ndim == 1:\n",
    "            action = action[None]\n",
    "        for i in range(action.shape[0]):\n",
    "            if torch.any(action < 0) or torch.any(action > 1):\n",
    "                continue\n",
    "            # Correct calculation for pixel positions considering the normalized action coordinates\n",
    "            pixel_x = int(action[i, 0].item() * (image.shape[-1] - 1))\n",
    "            pixel_y = int(action[i, 1].item() * (image.shape[-2] - 1))\n",
    "            # Highlight the click location with a red color\n",
    "            clicked_image_frame[max(0, pixel_y-5):pixel_y+6, \n",
    "                                max(0, pixel_x-5):pixel_x+6, \n",
    "                                :] = np.array([1.0, 1.0, 1.0])\n",
    "        if torch.any(gt_image != 0):\n",
    "            image_list = [old_image_frame, clicked_image_frame, new_image_frame, ground_truth_image_frame]\n",
    "            image_titles = ['Previous Frame', 'Click Location', 'New Sample', 'Ground Truth']\n",
    "            if gt_diff_flag:\n",
    "                image_list.append(exclamation_mark_image)\n",
    "                image_titles.append('Difference')\n",
    "            show_img(image_list,\n",
    "                     figure_title=f'Performing action {(action if action.ndim == 1 else action[0]).squeeze().cpu().numpy()}',\n",
    "                     titles=image_titles)\n",
    "        else:\n",
    "            show_img([old_image_frame, clicked_image_frame, new_image_frame],\n",
    "                    figure_title=f'Performing action {(action if action.ndim == 1 else action[0]).squeeze().cpu().numpy()}',\n",
    "                    titles=['Previous Frame', 'Click Location', 'New Sample'])\n",
    "    return new_image, new_latents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = GridworldDataset(\n",
    "\tdata_folder=data_folder, split='val_indep', return_targets=False, single_image=True, return_latents=True, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=1.0,)\n",
    "for elem in val_dataset:\n",
    "    print(elem[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.datasets import GridworldDataset\n",
    "data_folder = '/home/john/PhD/BISCUIT/data/gridworld_simplified_5c'\n",
    "# val_seq_dataset = GridworldDataset(\n",
    "# data_folder=data_folder, split='val', return_targets=True, return_robot_state=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=0.01)\n",
    "test_seq_dataset = GridworldDataset(\n",
    "        data_folder=data_folder, split='train', return_targets=True, return_latents=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=True, subsample_percentage=0.01)\n",
    "val_indep_dataset = GridworldDataset(\n",
    "         data_folder=data_folder, split='val_indep', return_targets=True, single_image=True, return_latents=True, triplet=False, seq_len=1, cluster=False, return_text=True, subsample_percentage=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "test_elem = test_seq_dataset[0]\n",
    "input_ids, token_type_ids, attention_mask = test_elem[-4:-1]\n",
    "tokenizer.decode(input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def reverseEnum(data: list):\n",
    "    for i in range(len(data)-1, -1, -1):\n",
    "        yield (i, data[i])\n",
    "\n",
    "for i, frame_seq in enumerate(test_seq_dataset):\n",
    "        print(i)\n",
    "        action = torch.tensor(frame_seq[1]).to(device).squeeze()\n",
    "        interventions = torch.tensor(frame_seq[2]).to(device)\n",
    "        causals = torch.tensor(frame_seq[3]).to(device)\n",
    "        tokenized_description = {'input_ids': torch.tensor(frame_seq[3]).to(device), 'token_type_ids': torch.tensor(frame_seq[4]).to(device), 'attention_mask': torch.tensor(frame_seq[5]).to(device)}\n",
    "        print(causals)\n",
    "        print(interventions)\n",
    "        # intv_targets = torch.tensor(frame_seq[2]).to(device)\n",
    "        intv_targets = None\n",
    "        plot_images = True\n",
    "        new_image, new_latents = next_step_prediction(model, frame_seq[0][0], action, gt_image=frame_seq[0][1], plot_images=plot_images, intv_targets=intv_targets, tokenized_description=tokenized_description, text_only=False, N=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq_dataset[94][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "frame1 = test_seq_dataset[idx][0][0]\n",
    "frame2 = test_seq_dataset[idx][0][1]\n",
    "tokenized_description = {'input_ids': torch.tensor(test_seq_dataset[idx][3]).to(device), 'token_type_ids': torch.tensor(test_seq_dataset[idx][4]).to(device), 'attention_mask': torch.tensor(test_seq_dataset[idx][5]).to(device)}\n",
    "zdt = model.flow(model.autoencoder.encoder(frame1[None]))\n",
    "zdtplus1 = model.flow(model.autoencoder.encoder(frame2[None]))\n",
    "zdtplus1prior = model.prior_t1.sample(zdt[0], action=torch.tensor(test_seq_dataset[94][1]).to(device), num_samples=1, intv_targets=None, tokenized_description=tokenized_description)\n",
    "print(zdt)\n",
    "print(zdtplus1)\n",
    "zdtplus1flow = model.flow.reverse(zdtplus1[0])\n",
    "zdtplus1flow = model.autoencoder.decoder(zdtplus1flow)\n",
    "zdtplus1flow = (zdtplus1flow + 1) / 2\n",
    "zdtplus1prior_ = model.flow.reverse(zdtplus1prior[0])\n",
    "zdtplus1prior_ = model.autoencoder.decoder(zdtplus1prior_)\n",
    "zdtplus1prior_ = (zdtplus1prior_ + 1) / 2\n",
    "from matplotlib import pyplot as plt\n",
    "# plt.imshow(zt.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "# plt.imshow(zdtplus1prior_.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "plt.imshow(zdtplus1flow.squeeze().cpu().detach().numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zdtplus1prior[0] - zdtplus1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, frame_seq in enumerate(val_seq_dataset):\n",
    "    # Stack the images at each step to form a sequence\n",
    "    frames, actions = frame_seq\n",
    "    frames = frames.to(device)\n",
    "    actions = torch.from_numpy(actions).to(device)\n",
    "    # interventions = interventions.to(device)\n",
    "    # causals = causals.to(device)\n",
    "    # Perform the next step prediction\n",
    "    new_image, new_latents = next_step_prediction(frames[0], actions, gt_image=frames[1], intv_targets=None, plot_images=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "latents = None\n",
    "image = test_seq_dataset[0][0][0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "load_text = ax.text(image.shape[-1]//2, \n",
    "                image.shape[-2]//2, \n",
    "                'Loading...', \n",
    "                fontsize='x-large',\n",
    "                weight='bold',\n",
    "                va='center',\n",
    "                ha='center',\n",
    "                backgroundcolor=(1.0, 0.8, 0.8))\n",
    "load_text.set_visible(False)\n",
    "ax.axis('off')\n",
    "\n",
    "def onclick(event):\n",
    "    print('click')\n",
    "    global image, latents\n",
    "    load_text.set_visible(True)\n",
    "    fig.canvas.draw()\n",
    "    ix, iy = event.xdata, event.ydata\n",
    "    # ix = (ix / image.shape[-1] - 0.5) * 2.0\n",
    "    # iy = (iy / image.shape[-2] - 0.5) * 2.0\n",
    "    print(f'Clicked at x={ix}, y={iy}')\n",
    "    image, latents = next_step_prediction(model, image=image,\n",
    "                                            action=torch.tensor([iy, ix], \n",
    "                                                                    dtype=torch.float32,\n",
    "                                                                    device=device),\n",
    "                                            latents=latents,\n",
    "                                            plot_images=False)\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    load_text.set_visible(False)\n",
    "    fig.canvas.draw()\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = test_seq_dataset[5][0][0]\n",
    "latents = None\n",
    "action_sequence = [\n",
    "    (0.42857143, 1.),\n",
    "    (0., 0.),\n",
    "    (-0.14285714, -0.14285714),\n",
    "    (0.42857143, 1.),\n",
    "    (0., 0.),\n",
    "]\n",
    "\n",
    "for i, action in enumerate(action_sequence):\n",
    "    image, latents = next_step_prediction(model,\n",
    "                                          image=image, \n",
    "                                          action=torch.tensor(action, device=device), \n",
    "                                          latents=latents,\n",
    "                                          gt_image=test_seq_dataset[5][0][1],\n",
    "                                          plot_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "# from open_clip import get_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "# tokenizer = get_tokenizer('hf-hub:timm/ViT-B-16-SigLIP')\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "action = torch.tensor((-0.14285714, -0.14285714)).to(device)\n",
    "\n",
    "# Assuming test_seq_dataset, next_step_prediction, and device are defined elsewhere\n",
    "latents = None\n",
    "image = test_seq_dataset[0][0][0]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "load_text = ax.text(image.shape[-1]//2, \n",
    "                    image.shape[-2]//2, \n",
    "                    'Enter action and press \"Update\"', \n",
    "                    fontsize='x-large',\n",
    "                    weight='bold',\n",
    "                    va='center',\n",
    "                    ha='center',\n",
    "                    backgroundcolor=(1.0, 0.8, 0.8))\n",
    "load_text.set_visible(True)  # Initially visible to prompt for input\n",
    "ax.axis('off')\n",
    "\n",
    "# Text input for action\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type action here',\n",
    "    description='Action:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Button to trigger the update\n",
    "update_button = widgets.Button(\n",
    "    description='Update',\n",
    "    disabled=False,\n",
    "    button_style='',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to update based on action',\n",
    "    icon='check'  # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "def on_update_button_clicked(b):\n",
    "    global image, latents\n",
    "    text = text_input.value \n",
    "    \n",
    "    \n",
    "    print(f'Action: {text}') \n",
    "\n",
    "    # Update the image and latents based on the provided action\n",
    "    image, latents = next_step_prediction(model,\n",
    "                                          image=image,\n",
    "                                          action=action,  # Assuming the function now accepts a string\n",
    "                                          latents=latents,\n",
    "                                          plot_images=False,\n",
    "                                          text=text,\n",
    "                                          tokenizer=tokenizer,\n",
    "                                          text_only=True)\n",
    "                                          \n",
    "    \n",
    "    # Update the plot with the new image\n",
    "    ax.clear()  # Clear the previous image\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    load_text.set_visible(False)  # Hide the loading text\n",
    "    ax.axis('off')  # Hide axes again after redrawing\n",
    "    fig.canvas.draw()  # Refresh the figure\n",
    "\n",
    "update_button.on_click(on_update_button_clicked)\n",
    "\n",
    "# Display the widgets\n",
    "widgets.VBox([text_input, update_button])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(map(get_color_name, [eval(x.split('_')[1]) for x in causal_dicts[0].keys()]))\n",
    "# Replace the rgb values with the color names in the causal_dicts keys while keeping the rest of the key intact\n",
    "# i.e., 'obstacle_(255, 165, 0)_position_x' -> 'obstacle_orange_position_x'\n",
    "plt.imshow(frame1, origin='upper', interpolation='none')\n",
    "# Remove the axes and the white space around the image\n",
    "plt.gca().set_axis_off()\n",
    "# Remove the white space around the image\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame2, origin='upper', interpolation='none')\n",
    "# Remove the axes and the white space around the image\n",
    "plt.gca().set_axis_off()\n",
    "# Remove the white space around the image\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from models.ae import Autoencoder\n",
    "\n",
    "import io\n",
    "import base64\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from dash import Dash, dcc, html, Input, Output, no_update, callback\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "import io\n",
    "import base64\n",
    "import dash\n",
    "\n",
    "# Helper functions\n",
    "def np_image_to_base64(img):\n",
    "    \"\"\"Convert a NumPy array to a base64 encoded image.\"\"\"\n",
    "    img = Image.fromarray((img * 255).astype(np.uint8))\n",
    "    buffered = io.BytesIO()\n",
    "    img.save(buffered, format=\"PNG\")\n",
    "    return f\"data:image/png;base64,{base64.b64encode(buffered.getvalue()).decode()}\"\n",
    "\n",
    "# Main function to prepare data and create Dash app\n",
    "def create_visualizations_app(model, dataloader, perplexity=30, n_neighbors=15, min_dist=0.1, max_samples=500):\n",
    "    model.eval()\n",
    "    embeddings, images, frame_positions = [], [], []\n",
    "    sample_count = 0\n",
    "\n",
    "    # Collect data\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            imgs, positions, *_ = batch  # Update this based on your data structure\n",
    "            if imgs.dim() > 4:\n",
    "                imgs = imgs.view(-1, *imgs.shape[2:])\n",
    "                positions = positions.view(-1)\n",
    "\n",
    "            if sample_count + imgs.shape[0] > max_samples:\n",
    "                limit = max_samples - sample_count\n",
    "                imgs = imgs[:limit]\n",
    "                positions = positions[:limit]\n",
    "\n",
    "            embeddings_batch = model(imgs.to('cuda')).cpu().numpy()  # Ensure device compatibility\n",
    "            embeddings.append(embeddings_batch)\n",
    "            images.extend([np_image_to_base64(np.array(img.permute(1, 2, 0))) for img in imgs])\n",
    "            frame_positions.extend(positions.cpu().numpy())\n",
    "\n",
    "            sample_count += imgs.shape[0]\n",
    "            if sample_count >= max_samples:\n",
    "                break\n",
    "\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    colors = frame_positions / np.max(frame_positions)  # Normalize for color mapping\n",
    "\n",
    "    # Perform dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity).fit_transform(embeddings)\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=2)\n",
    "    umap_result = reducer.fit_transform(embeddings)\n",
    "\n",
    "    # Create Dash app\n",
    "    app = Dash(__name__)\n",
    "\n",
    "    app.layout = html.Div([\n",
    "        dcc.Graph(id=\"graph-tsne\", figure=create_figure(tsne, images, frame_positions, 't-SNE Visualization')),\n",
    "        dcc.Graph(id=\"graph-umap\", figure=create_figure(umap_result, images, frame_positions, 'UMAP Visualization')),\n",
    "        dcc.Tooltip(id=\"graph-tooltip\")\n",
    "    ])\n",
    "\n",
    "    @callback(\n",
    "        Output(\"graph-tooltip\", \"show\"),\n",
    "        Output(\"graph-tooltip\", \"bbox\"),\n",
    "        Output(\"graph-tooltip\", \"children\"),\n",
    "        Input(\"graph-tsne\", \"hoverData\"),\n",
    "        Input(\"graph-umap\", \"hoverData\")\n",
    "    )\n",
    "    def display_hover(hoverData_tsne, hoverData_umap):\n",
    "        ctx = dash.callback_context\n",
    "\n",
    "        if not ctx.triggered:\n",
    "            return False, no_update, no_update\n",
    "\n",
    "        hover_data = ctx.triggered[0][\"value\"][\"points\"][0]\n",
    "        bbox = hover_data[\"bbox\"]\n",
    "        num = hover_data[\"pointNumber\"]\n",
    "\n",
    "        children = html.Div([\n",
    "            html.Img(src=images[num], style={\"width\": \"100px\", 'display': 'block', 'margin': '0 auto'}),\n",
    "            # html.P(f\"Frame Position: {frame_positions[num]}\", style={'font-weight': 'bold'})\n",
    "        ])\n",
    "\n",
    "        return True, bbox, children\n",
    "\n",
    "    return app\n",
    "\n",
    "def create_figure(data, images, frame_positions, title):\n",
    "    fig = go.Figure(data=[go.Scatter(\n",
    "        x=data[:, 0],\n",
    "        y=data[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(size=5, color=frame_positions, colorscale='Viridis', showscale=True),\n",
    "        text=[f'' for img, pos in zip(images, frame_positions)],\n",
    "        hoverinfo='text'\n",
    "    )])\n",
    "    fig.update_layout(title=title, xaxis_title=f'{title} Dimension 1', yaxis_title=f'{title} Dimension 2')\n",
    "    return fig\n",
    "\n",
    "model = Autoencoder.load_from_checkpoint('/home/john/PhD/BISCUIT/pretrained_models/epoch=14-step=8325.ckpt')\n",
    "from experiments.datasets import GridworldDataset\n",
    "data_folder = '/home/john/PhD/BISCUIT/data_generation/data/gridworld_simplified_2c2b2l_noturn_noshufflecars_bfix_preintv'\n",
    "train_seq_dataset = GridworldDataset(\n",
    "        data_folder=data_folder, split='val', return_targets=True, return_latents=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=1.0, return_whole_episode=True)\n",
    "train_seq_dataloader = torch.utils.data.DataLoader(train_seq_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Run the app\n",
    "app = create_visualizations_app(model, train_seq_dataloader, perplexity=30, n_neighbors=100, min_dist=0.1, max_samples=300)\n",
    "app.run_server(debug=True, port=8050, host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
