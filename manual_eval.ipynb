{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_path)\n",
    "model['hyper_parameters']['autoencoder_checkpoint'] = '/home/john/PhD/BISCUIT/pretrained_models/AE_gridworld_simplified/AE_40l_64hid.ckpt'\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.biscuit_nf import BISCUITNF\n",
    "import torch\n",
    "from typing import List, Tuple, Dict, Any, Union, Optional\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "autoencoder_path = '/home/john/PhD/BISCUIT/pretrained_moels/AE_gridworld_simplified/AE_20l_64hid.ckpt'\n",
    "model_path = '/home/john/PhD/BISCUIT/pretrained_models/AE_gridworld_simplified/NF_text_only_cardinal.ckpt'\n",
    "model = BISCUITNF.load_from_checkpoint(model_path, autoencoder_path=autoencoder_path)\n",
    "model.to(device)\n",
    "model.freeze()\n",
    "_ = model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "encs_path = '/home/john/PhD/BISCUIT/pretrained_models/AE_gridworld_simplified/encodings/gridworld_simplified_5c_test_indep.pt'\n",
    "encs_path_drop_last_frame = '/home/john/PhD/BISCUIT/pretrained_models/AE_gridworld_simplified/encodings/gridworld_simplified_5c_drop_last_frame_test_indep.pt'\n",
    "t = torch.load(encs_path)\n",
    "t1 = torch.load(encs_path_drop_last_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "for i in range(50):\n",
    "    print(i)\n",
    "    img1 = t1[i]\n",
    "    img2 = t[i]\n",
    "    print(img1)\n",
    "    print(img2)\n",
    "    img1 = model.autoencoder.decoder(torch.from_numpy(img1).to(device).unsqueeze(0))\n",
    "    img1 = (img1 + 1) / 2\n",
    "    img2 = model.autoencoder.decoder(torch.from_numpy(img2).to(device).unsqueeze(0))\n",
    "    img2 = (img2 + 1) / 2\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(img1.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "    ax[1].imshow(img2.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_path = '/home/gkounto/BISCUIT/experiments/pretrained_models/AE_gridworld_small/encodings/gridworld_small_pre_intv_freeze_test.pt'\n",
    "encodings = torch.load(encodings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.datasets import GridworldDataset\n",
    "data_folder = '/home/john/PhD/BISCUIT/data/gridworld_simplified_5c'\n",
    "# val_seq_dataset = GridworldDataset(\n",
    "# data_folder=data_folder, split='val', return_targets=True, return_robot_state=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=0.01)\n",
    "train_seq_dataset = GridworldDataset(\n",
    "        data_folder=data_folder, split='train', return_targets=True, return_latents=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=True, subsample_percentage=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from data_generation.gridworld import Gridworld\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def format_causal_dict(causal_dict):\n",
    "    formatted_text = \"\"\n",
    "    for key, value in causal_dict.items():\n",
    "        formatted_text += f\"{key}: {value}\\n\"\n",
    "        if \"position_y\" in key:  # Add a line break after each entity's details\n",
    "            formatted_text += \"\\n\"\n",
    "    return formatted_text\n",
    "\n",
    "def visualize_episode(frames, causals, causal_keys, actions, action_descriptions, interventions):\n",
    "    for i in range(len(frames)):\n",
    "        # debug_causals = Gridworld.causal_vector_to_debug_dict(causal_keys, causals[i])\n",
    "        debug_causals = dict(zip(causal_keys, causals[i]))\n",
    "        formatted_causals = format_causal_dict(debug_causals)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax1 = plt.subplot(1, 2, 1)  # Frame subplot\n",
    "        ax1.imshow(frames[i])\n",
    "        ax1.set_title(f\"Frame {i+1}\")\n",
    "        ax1.axis('off')\n",
    "\n",
    "        ax2 = plt.subplot(1, 2, 2)  # Textual information subplot\n",
    "        ax2.axis('off')\n",
    "        text_info = (\n",
    "            f\"Step {i+1}:\\n\\n\"\n",
    "            f\"Causals (Formatted):\\n{formatted_causals}\\n\"\n",
    "            f\"Action: {actions[i]}\\n\"\n",
    "            f\"Action Description: {action_descriptions[i]}\\n\"\n",
    "            f\"Interventions: {interventions[i]}\"\n",
    "        )\n",
    "        ax2.text(0, 1, text_info, ha='left', va='top', fontsize=8, wrap=True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "a = np.load('/home/john/PhD/BISCUIT/data_generation/data/gridworld_simplified_2c2b2l_noturn_noshufflecars/check/gridworld_episode_28.npz')\n",
    "causal_keys = json.load(open('/home/john/PhD/BISCUIT/data_generation/data/gridworld_simplified_2c2b2l_noturn_noshufflecars/check_metadata.json'))['flattened_causals']\n",
    "frames, causals, actions, interventions, action_descriptions = a['frames'], a['causals'], a['actions'], a['interventions'], a['action_descriptions']\n",
    "visualize_episode(frames, causals, causal_keys, actions, action_descriptions, interventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "# Assume Gridworld, format_causal_dict, and the dataset class are defined elsewhere\n",
    "\n",
    "def visualize_episodes(dataset, causal_keys, N=1):\n",
    "    for episode_idx in range(min(N, len(dataset))):  # Loop through N episodes or the total dataset length\n",
    "        frame_seq = dataset[episode_idx]  # Get the episode data\n",
    "        frames, actions, interventions, causals = frame_seq\n",
    "        \n",
    "        for i in range(frames.shape[0] - 1):  # Iterate through each step, excluding the last frame\n",
    "            # Map causals to keys for the current frame\n",
    "            debug_causals = dict(zip(causal_keys, causals[i]))\n",
    "            formatted_causals = format_causal_dict(debug_causals)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax1 = plt.subplot(1, 2, 1)  # Frame subplot\n",
    "            # Adjust image normalization if necessary\n",
    "            img = (frames[i].permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0  # Normalize if required\n",
    "            ax1.imshow(img)\n",
    "            ax1.set_title(f\"Episode {episode_idx + 1}, Frame {i + 1}\")\n",
    "            ax1.axis('off')\n",
    "\n",
    "            ax2 = plt.subplot(1, 2, 2)  # Textual information subplot\n",
    "            ax2.axis('off')\n",
    "            text_info = (\n",
    "                f\"Episode {episode_idx + 1}, Step {i + 1}:\\n\\n\"\n",
    "                f\"Causals (Formatted):\\n{formatted_causals}\\n\"\n",
    "                f\"Action: {actions[i]}\\n\"\n",
    "                f\"Interventions: {interventions[i]}\"\n",
    "            )\n",
    "            ax2.text(0, 1, text_info, ha='left', va='top', fontsize=8, wrap=True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Example usage:\n",
    "causal_keys = json.load(open('/home/gkounto/BISCUIT/data_generation/data/gridworld_simplified/train_metadata.json'))['flattened_causals']\n",
    "# Assuming train_seq_dataset is defined and loaded\n",
    "visualize_episodes(train_seq_dataset, causal_keys, N=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_img(imgs: Any,\n",
    "             figure_title: str = None,\n",
    "             titles: Optional[list] = None):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axes = plt.subplots(1, len(imgs), figsize=(len(imgs) * 3.5, 3.5))\n",
    "    if len(imgs) == 1:\n",
    "        axes = [axes]\n",
    "    for i, ax in enumerate(axes):\n",
    "        if len(imgs[i].shape) == 3 and imgs[i].shape[0] in [3,4]:\n",
    "            imgs[i] = imgs[i].permute(1, 2, 0)\n",
    "        if isinstance(imgs[i], torch.Tensor):\n",
    "            imgs[i] = imgs[i].detach().cpu().numpy()\n",
    "        ax.imshow(imgs[i])\n",
    "        ax.axis('off')\n",
    "        if titles is not None and i < len(titles):\n",
    "            ax.set_title(titles[i], weight='bold')\n",
    "    if figure_title is not None:\n",
    "        fig.suptitle(figure_title, weight='bold', size=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_exclamation_mark_image(size=256, background_color=(1, 1, 1), mark_color=(1, 0, 0), mark_thickness=10, mark_height=100, dot_radius=10):\n",
    "    \"\"\"\n",
    "    Creates an image with a red exclamation mark in the center on a white background.\n",
    "\n",
    "    Parameters:\n",
    "    - size (int): The size of the image (width and height). Default is 256.\n",
    "    - background_color (tuple): The RGB color of the background in the range 0 to 1. Default is white.\n",
    "    - mark_color (tuple): The RGB color of the exclamation mark in the range 0 to 1. Default is red.\n",
    "    - mark_thickness (int): The thickness of the line part of the exclamation mark. Default is 10.\n",
    "    - mark_height (int): The height of the line part of the exclamation mark. Default is 100.\n",
    "    - dot_radius (int): The radius of the dot part of the exclamation mark. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The generated image as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Create a size x size x 3 array with the background color\n",
    "    image = np.ones((size, size, 3)) * np.array(background_color)\n",
    "\n",
    "    # Define the center position\n",
    "    center_x, center_y = size // 2, size // 2\n",
    "\n",
    "    # Draw the line part of the exclamation mark\n",
    "    for x in range(center_x - mark_thickness // 2, center_x + mark_thickness // 2):\n",
    "        for y in range(center_y - mark_height // 2, center_y + mark_height // 2 - dot_radius * 2):\n",
    "            image[y, x] = mark_color\n",
    "\n",
    "    # Draw the dot part of the exclamation mark\n",
    "    for x in range(image.shape[1]):\n",
    "        for y in range(image.shape[0]):\n",
    "            if (x - center_x) ** 2 + (y - (center_y + mark_height // 2 + dot_radius)) ** 2 <= dot_radius ** 2:\n",
    "                image[y, x] = mark_color\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def next_step_prediction(\n",
    "        model: BISCUITNF,\n",
    "        image: torch.Tensor,\n",
    "        action: torch.Tensor,\n",
    "        gt_image: torch.Tensor = torch.zeros(1),\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        plot_images: bool = True,\n",
    "        intv_targets: Optional[torch.Tensor] = None,\n",
    "        text: Optional[str] = None,\n",
    "        tokenizer: Optional[Any] = None,\n",
    "        text_only: bool = False,\n",
    "        N: int = 8\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if latents is None:\n",
    "        input_image = (image * 2.0) - 1.0\n",
    "        latents = model.autoencoder.encoder(input_image[None])\n",
    "        latents, _ = model.flow.forward(latents)\n",
    "    if text is not None:\n",
    "        tokenized_description = tokenizer.tokenizer(text, return_token_type_ids=True, padding='max_length', max_length=64)\n",
    "        input_ids = torch.tensor(tokenized_description['input_ids']).to(device)\n",
    "        token_type_ids = torch.tensor(tokenized_description['token_type_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokenized_description['attention_mask']).to(device)\n",
    "        tokenized_description = {'input_ids': input_ids, 'token_type_ids': token_type_ids, 'attention_mask': attention_mask}    \n",
    "    new_latents, _ = model.prior_t1.sample(latents, action[None], num_samples=1, intv_targets=intv_targets, tokenized_description=tokenized_description)\n",
    "    new_latents = new_latents.squeeze(1)\n",
    "    new_encodings = model.flow.reverse(new_latents)\n",
    "    new_image = model.autoencoder.decoder(new_encodings)[0]\n",
    "    new_image = (new_image + 1.0) / 2.0\n",
    "    if plot_images:\n",
    "        gt_diff_flag = False\n",
    "        new_image_frame = new_image.permute(1, 2, 0).cpu().numpy()\n",
    "        if latents is None:\n",
    "            old_image_frame = (image.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0\n",
    "        else:\n",
    "            old_image_frame = image.permute(1, 2, 0).cpu().numpy()\n",
    "        clicked_image_frame = np.copy(old_image_frame)\n",
    "        ground_truth_image_frame = (gt_image.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0\n",
    "        difference = np.abs(new_image_frame - ground_truth_image_frame)\n",
    "        print(f\"Mean absolute difference between the new image and the ground truth: {difference.mean()}\")\n",
    "        if difference.mean() > 0.001:\n",
    "            print(\"The new image is not close to the ground truth\")\n",
    "            gt_diff_flag = True\n",
    "            exclamation_mark_image = create_exclamation_mark_image()\n",
    "        if action.ndim == 1:\n",
    "            action = action[None]\n",
    "        for i in range(action.shape[0]):\n",
    "            if torch.any(action < 0) or torch.any(action > 1):\n",
    "                continue\n",
    "            # Correct calculation for pixel positions considering the normalized action coordinates\n",
    "            pixel_x = int(action[i, 0].item() * (image.shape[-1] - 1))\n",
    "            pixel_y = int(action[i, 1].item() * (image.shape[-2] - 1))\n",
    "            # Highlight the click location with a red color\n",
    "            clicked_image_frame[max(0, pixel_y-5):pixel_y+6, \n",
    "                                max(0, pixel_x-5):pixel_x+6, \n",
    "                                :] = np.array([1.0, 1.0, 1.0])\n",
    "        if torch.any(gt_image != 0):\n",
    "            image_list = [old_image_frame, clicked_image_frame, new_image_frame, ground_truth_image_frame]\n",
    "            image_titles = ['Previous Frame', 'Click Location', 'New Sample', 'Ground Truth']\n",
    "            if gt_diff_flag:\n",
    "                image_list.append(exclamation_mark_image)\n",
    "                image_titles.append('Difference')\n",
    "            show_img(image_list,\n",
    "                     figure_title=f'Performing action {(action if action.ndim == 1 else action[0]).squeeze().cpu().numpy()}',\n",
    "                     titles=image_titles)\n",
    "        else:\n",
    "            show_img([old_image_frame, clicked_image_frame, new_image_frame],\n",
    "                    figure_title=f'Performing action {(action if action.ndim == 1 else action[0]).squeeze().cpu().numpy()}',\n",
    "                    titles=['Previous Frame', 'Click Location', 'New Sample'])\n",
    "    return new_image, new_latents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = GridworldDataset(\n",
    "\tdata_folder=data_folder, split='val_indep', return_targets=False, single_image=True, return_latents=True, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=1.0,)\n",
    "for elem in val_dataset:\n",
    "    print(elem[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.datasets import GridworldDataset\n",
    "data_folder = '/home/john/PhD/BISCUIT/data/gridworld_simplified_5c'\n",
    "# val_seq_dataset = GridworldDataset(\n",
    "# data_folder=data_folder, split='val', return_targets=True, return_robot_state=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=0.01)\n",
    "test_seq_dataset = GridworldDataset(\n",
    "        data_folder=data_folder, split='test', return_targets=True, return_latents=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=0.01)\n",
    "val_indep_dataset = GridworldDataset(\n",
    "         data_folder=data_folder, split='val_indep', return_targets=True, single_image=True, return_latents=True, triplet=False, seq_len=1, cluster=False, return_text=False, subsample_percentage=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def reverseEnum(data: list):\n",
    "    for i in range(len(data)-1, -1, -1):\n",
    "        yield (i, data[i])\n",
    "\n",
    "for i, frame_seq in enumerate(test_seq_dataset):\n",
    "        print(i)\n",
    "        action = torch.tensor(frame_seq[1]).to(device).squeeze()\n",
    "        interventions = torch.tensor(frame_seq[2]).to(device)\n",
    "        causals = torch.tensor(frame_seq[3]).to(device)\n",
    "        print(causals)\n",
    "        print(interventions)\n",
    "        # intv_targets = torch.tensor(frame_seq[2]).to(device)\n",
    "        intv_targets = None\n",
    "        plot_images = True\n",
    "        new_image, new_latents = next_step_prediction(model, frame_seq[0][0], action, gt_image=frame_seq[0][1], plot_images=plot_images, intv_targets=intv_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, frame_seq in enumerate(val_seq_dataset):\n",
    "    # Stack the images at each step to form a sequence\n",
    "    frames, actions = frame_seq\n",
    "    frames = frames.to(device)\n",
    "    actions = torch.from_numpy(actions).to(device)\n",
    "    # interventions = interventions.to(device)\n",
    "    # causals = causals.to(device)\n",
    "    # Perform the next step prediction\n",
    "    new_image, new_latents = next_step_prediction(frames[0], actions, gt_image=frames[1], intv_targets=None, plot_images=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "latents = None\n",
    "image = test_seq_dataset[0][0][0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "load_text = ax.text(image.shape[-1]//2, \n",
    "                image.shape[-2]//2, \n",
    "                'Loading...', \n",
    "                fontsize='x-large',\n",
    "                weight='bold',\n",
    "                va='center',\n",
    "                ha='center',\n",
    "                backgroundcolor=(1.0, 0.8, 0.8))\n",
    "load_text.set_visible(False)\n",
    "ax.axis('off')\n",
    "\n",
    "def onclick(event):\n",
    "    print('click')\n",
    "    global image, latents\n",
    "    load_text.set_visible(True)\n",
    "    fig.canvas.draw()\n",
    "    ix, iy = event.xdata, event.ydata\n",
    "    # ix = (ix / image.shape[-1] - 0.5) * 2.0\n",
    "    # iy = (iy / image.shape[-2] - 0.5) * 2.0\n",
    "    print(f'Clicked at x={ix}, y={iy}')\n",
    "    image, latents = next_step_prediction(model, image=image,\n",
    "                                            action=torch.tensor([iy, ix], \n",
    "                                                                    dtype=torch.float32,\n",
    "                                                                    device=device),\n",
    "                                            latents=latents,\n",
    "                                            plot_images=False)\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    load_text.set_visible(False)\n",
    "    fig.canvas.draw()\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = test_seq_dataset[5][0][0]\n",
    "latents = None\n",
    "action_sequence = [\n",
    "    (0.42857143, 1.),\n",
    "    (0., 0.),\n",
    "    (-0.14285714, -0.14285714),\n",
    "    (0.42857143, 1.),\n",
    "    (0., 0.),\n",
    "]\n",
    "\n",
    "for i, action in enumerate(action_sequence):\n",
    "    image, latents = next_step_prediction(model,\n",
    "                                          image=image, \n",
    "                                          action=torch.tensor(action, device=device), \n",
    "                                          latents=latents,\n",
    "                                          gt_image=test_seq_dataset[5][0][1],\n",
    "                                          plot_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "# Assume Gridworld, format_causal_dict, and the dataset class are defined elsewhere\n",
    "\n",
    "def visualize_episodes(dataset, causal_keys, N=1):\n",
    "    for episode_idx in range(min(N, len(dataset))):  # Loop through N episodes or the total dataset length\n",
    "        frame_seq = dataset[episode_idx]  # Get the episode data\n",
    "        frames, actions, interventions, causals = frame_seq\n",
    "        \n",
    "        for i in range(frames.shape[0] - 1):  # Iterate through each step, excluding the last frame\n",
    "            # Map causals to keys for the current frame\n",
    "            debug_causals = dict(zip(causal_keys, causals[i]))\n",
    "            print(debug_causals)\n",
    "            formatted_causals = format_causal_dict(debug_causals)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax1 = plt.subplot(1, 2, 1)  # Frame subplot\n",
    "            # Adjust image normalization if necessary\n",
    "            img = (frames[i].permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0  # Normalize if required\n",
    "            ax1.imshow(img)\n",
    "            ax1.set_title(f\"Episode {episode_idx + 1}, Frame {i + 1}\")\n",
    "            ax1.axis('off')\n",
    "\n",
    "            ax2 = plt.subplot(1, 2, 2)  # Textual information subplot\n",
    "            ax2.axis('off')\n",
    "            text_info = (\n",
    "                f\"Episode {episode_idx + 1}, Step {i + 1}:\\n\\n\"\n",
    "                f\"Causals (Formatted):\\n{formatted_causals}\\n\"\n",
    "                f\"Action: {actions[i]}\\n\"\n",
    "                f\"Interventions: {interventions[i]}\"\n",
    "            )\n",
    "            ax2.text(0, 1, text_info, ha='left', va='top', fontsize=8, wrap=True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Example usage:\n",
    "causal_keys = json.load(open('/home/john/PhD/BISCUIT/data/gridworld_simplified_5c/train_metadata.json'))['flattened_causals']\n",
    "# Assuming train_seq_dataset is defined and loaded\n",
    "visualize_episodes(train_seq_dataset, causal_keys, N=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_img(imgs: Any,\n",
    "             figure_title: str = None,\n",
    "             titles: Optional[list] = None):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axes = plt.subplots(1, len(imgs), figsize=(len(imgs) * 3.5, 3.5))\n",
    "    if len(imgs) == 1:\n",
    "        axes = [axes]\n",
    "    for i, ax in enumerate(axes):\n",
    "        if len(imgs[i].shape) == 3 and imgs[i].shape[0] in [3,4]:\n",
    "            imgs[i] = imgs[i].permute(1, 2, 0)\n",
    "        if isinstance(imgs[i], torch.Tensor):\n",
    "            imgs[i] = imgs[i].detach().cpu().numpy()\n",
    "        ax.imshow(imgs[i])\n",
    "        ax.axis('off')\n",
    "        if titles is not None and i < len(titles):\n",
    "            ax.set_title(titles[i], weight='bold')\n",
    "    if figure_title is not None:\n",
    "        fig.suptitle(figure_title, weight='bold', size=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_exclamation_mark_image(size=256, background_color=(1, 1, 1), mark_color=(1, 0, 0), mark_thickness=10, mark_height=100, dot_radius=10):\n",
    "    \"\"\"\n",
    "    Creates an image with a red exclamation mark in the center on a white background.\n",
    "\n",
    "    Parameters:\n",
    "    - size (int): The size of the image (width and height). Default is 256.\n",
    "    - background_color (tuple): The RGB color of the background in the range 0 to 1. Default is white.\n",
    "    - mark_color (tuple): The RGB color of the exclamation mark in the range 0 to 1. Default is red.\n",
    "    - mark_thickness (int): The thickness of the line part of the exclamation mark. Default is 10.\n",
    "    - mark_height (int): The height of the line part of the exclamation mark. Default is 100.\n",
    "    - dot_radius (int): The radius of the dot part of the exclamation mark. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The generated image as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Create a size x size x 3 array with the background color\n",
    "    image = np.ones((size, size, 3)) * np.array(background_color)\n",
    "\n",
    "    # Define the center position\n",
    "    center_x, center_y = size // 2, size // 2\n",
    "\n",
    "    # Draw the line part of the exclamation mark\n",
    "    for x in range(center_x - mark_thickness // 2, center_x + mark_thickness // 2):\n",
    "        for y in range(center_y - mark_height // 2, center_y + mark_height // 2 - dot_radius * 2):\n",
    "            image[y, x] = mark_color\n",
    "\n",
    "    # Draw the dot part of the exclamation mark\n",
    "    for x in range(image.shape[1]):\n",
    "        for y in range(image.shape[0]):\n",
    "            if (x - center_x) ** 2 + (y - (center_y + mark_height // 2 + dot_radius)) ** 2 <= dot_radius ** 2:\n",
    "                image[y, x] = mark_color\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def next_step_prediction(\n",
    "        model: BISCUITNF,\n",
    "        image: torch.Tensor,\n",
    "        action: torch.Tensor,\n",
    "        gt_image: torch.Tensor = torch.zeros(1),\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        plot_images: bool = True,\n",
    "        intv_targets: Optional[torch.Tensor] = None,\n",
    "        text: Optional[str] = None,\n",
    "        tokenizer: Optional[Any] = None,\n",
    "        text_only: bool = False,\n",
    "        tokenized_description=None,\n",
    "        N: int = 8\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if latents is None:\n",
    "        input_image = (image * 2.0) - 1.0\n",
    "        latents = model.autoencoder.encoder(input_image[None])\n",
    "        latents, _ = model.flow.forward(latents)\n",
    "    # tokenized_description = None\n",
    "    if text is not None:\n",
    "        tokenized_description = tokenizer(text, return_token_type_ids=True, padding='max_length', max_length=64)\n",
    "        input_ids = torch.tensor(tokenized_description['input_ids']).to(device)\n",
    "        token_type_ids = torch.tensor(tokenized_description['token_type_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokenized_description['attention_mask']).to(device)\n",
    "        tokenized_description = {'input_ids': input_ids, 'token_type_ids': token_type_ids, 'attention_mask': attention_mask}    \n",
    "    new_latents, _ = model.prior_t1.sample(latents, action[None], num_samples=1, intv_targets=intv_targets, tokenized_description=tokenized_description)\n",
    "    new_latents = new_latents.squeeze(1)\n",
    "    new_encodings = model.flow.reverse(new_latents)\n",
    "    new_image = model.autoencoder.decoder(new_encodings)[0]\n",
    "    new_image = (new_image + 1.0) / 2.0\n",
    "    if plot_images:\n",
    "        gt_diff_flag = False\n",
    "        new_image_frame = new_image.permute(1, 2, 0).cpu().numpy()\n",
    "        if latents is None:\n",
    "            old_image_frame = (image.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0\n",
    "        else:\n",
    "            old_image_frame = image.permute(1, 2, 0).cpu().numpy()\n",
    "        clicked_image_frame = np.copy(old_image_frame)\n",
    "        ground_truth_image_frame = (gt_image.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0\n",
    "        difference = np.abs(new_image_frame - ground_truth_image_frame)\n",
    "        print(f\"Mean absolute difference between the new image and the ground truth: {difference.mean()}\")\n",
    "        if difference.mean() > 0.001:\n",
    "            print(\"The new image is not close to the ground truth\")\n",
    "            gt_diff_flag = True\n",
    "            exclamation_mark_image = create_exclamation_mark_image()\n",
    "        if action.ndim == 1:\n",
    "            action = action[None]\n",
    "        for i in range(action.shape[0]):\n",
    "            if torch.any(action < 0) or torch.any(action > 1):\n",
    "                continue\n",
    "            # Correct calculation for pixel positions considering the normalized action coordinates\n",
    "            pixel_x = int(action[i, 0].item() * (image.shape[-1] - 1))\n",
    "            pixel_y = int(action[i, 1].item() * (image.shape[-2] - 1))\n",
    "            # Highlight the click location with a red color\n",
    "            clicked_image_frame[max(0, pixel_y-5):pixel_y+6, \n",
    "                                max(0, pixel_x-5):pixel_x+6, \n",
    "                                :] = np.array([1.0, 1.0, 1.0])\n",
    "        if torch.any(gt_image != 0):\n",
    "            image_list = [old_image_frame, clicked_image_frame, new_image_frame, ground_truth_image_frame]\n",
    "            image_titles = ['Previous Frame', 'Click Location', 'New Sample', 'Ground Truth']\n",
    "            if gt_diff_flag:\n",
    "                image_list.append(exclamation_mark_image)\n",
    "                image_titles.append('Difference')\n",
    "            show_img(image_list,\n",
    "                     figure_title=f'Performing action {(action if action.ndim == 1 else action[0]).squeeze().cpu().numpy()}',\n",
    "                     titles=image_titles)\n",
    "        else:\n",
    "            show_img([old_image_frame, clicked_image_frame, new_image_frame],\n",
    "                    figure_title=f'Performing action {(action if action.ndim == 1 else action[0]).squeeze().cpu().numpy()}',\n",
    "                    titles=['Previous Frame', 'Click Location', 'New Sample'])\n",
    "    return new_image, new_latents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = GridworldDataset(\n",
    "\tdata_folder=data_folder, split='val_indep', return_targets=False, single_image=True, return_latents=True, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=1.0,)\n",
    "for elem in val_dataset:\n",
    "    print(elem[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.datasets import GridworldDataset\n",
    "data_folder = '/home/john/PhD/BISCUIT/data/gridworld_simplified_5c'\n",
    "# val_seq_dataset = GridworldDataset(\n",
    "# data_folder=data_folder, split='val', return_targets=True, return_robot_state=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=0.01)\n",
    "test_seq_dataset = GridworldDataset(\n",
    "        data_folder=data_folder, split='train', return_targets=True, return_latents=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=True, subsample_percentage=0.01)\n",
    "val_indep_dataset = GridworldDataset(\n",
    "         data_folder=data_folder, split='val_indep', return_targets=True, single_image=True, return_latents=True, triplet=False, seq_len=1, cluster=False, return_text=True, subsample_percentage=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "test_elem = test_seq_dataset[0]\n",
    "input_ids, token_type_ids, attention_mask = test_elem[-4:-1]\n",
    "tokenizer.decode(input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def reverseEnum(data: list):\n",
    "    for i in range(len(data)-1, -1, -1):\n",
    "        yield (i, data[i])\n",
    "\n",
    "for i, frame_seq in enumerate(test_seq_dataset):\n",
    "        print(i)\n",
    "        action = torch.tensor(frame_seq[1]).to(device).squeeze()\n",
    "        interventions = torch.tensor(frame_seq[2]).to(device)\n",
    "        causals = torch.tensor(frame_seq[3]).to(device)\n",
    "        tokenized_description = {'input_ids': torch.tensor(frame_seq[3]).to(device), 'token_type_ids': torch.tensor(frame_seq[4]).to(device), 'attention_mask': torch.tensor(frame_seq[5]).to(device)}\n",
    "        print(causals)\n",
    "        print(interventions)\n",
    "        # intv_targets = torch.tensor(frame_seq[2]).to(device)\n",
    "        intv_targets = None\n",
    "        plot_images = True\n",
    "        new_image, new_latents = next_step_prediction(model, frame_seq[0][0], action, gt_image=frame_seq[0][1], plot_images=plot_images, intv_targets=intv_targets, tokenized_description=tokenized_description, text_only=False, N=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq_dataset[94][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "frame1 = test_seq_dataset[idx][0][0]\n",
    "frame2 = test_seq_dataset[idx][0][1]\n",
    "tokenized_description = {'input_ids': torch.tensor(test_seq_dataset[idx][3]).to(device), 'token_type_ids': torch.tensor(test_seq_dataset[idx][4]).to(device), 'attention_mask': torch.tensor(test_seq_dataset[idx][5]).to(device)}\n",
    "zdt = model.flow(model.autoencoder.encoder(frame1[None]))\n",
    "zdtplus1 = model.flow(model.autoencoder.encoder(frame2[None]))\n",
    "zdtplus1prior = model.prior_t1.sample(zdt[0], action=torch.tensor(test_seq_dataset[94][1]).to(device), num_samples=1, intv_targets=None, tokenized_description=tokenized_description)\n",
    "print(zdt)\n",
    "print(zdtplus1)\n",
    "zdtplus1flow = model.flow.reverse(zdtplus1[0])\n",
    "zdtplus1flow = model.autoencoder.decoder(zdtplus1flow)\n",
    "zdtplus1flow = (zdtplus1flow + 1) / 2\n",
    "zdtplus1prior_ = model.flow.reverse(zdtplus1prior[0])\n",
    "zdtplus1prior_ = model.autoencoder.decoder(zdtplus1prior_)\n",
    "zdtplus1prior_ = (zdtplus1prior_ + 1) / 2\n",
    "from matplotlib import pyplot as plt\n",
    "# plt.imshow(zt.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "# plt.imshow(zdtplus1prior_.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "plt.imshow(zdtplus1flow.squeeze().cpu().detach().numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zdtplus1prior[0] - zdtplus1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, frame_seq in enumerate(val_seq_dataset):\n",
    "    # Stack the images at each step to form a sequence\n",
    "    frames, actions = frame_seq\n",
    "    frames = frames.to(device)\n",
    "    actions = torch.from_numpy(actions).to(device)\n",
    "    # interventions = interventions.to(device)\n",
    "    # causals = causals.to(device)\n",
    "    # Perform the next step prediction\n",
    "    new_image, new_latents = next_step_prediction(frames[0], actions, gt_image=frames[1], intv_targets=None, plot_images=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "latents = None\n",
    "image = test_seq_dataset[0][0][0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "load_text = ax.text(image.shape[-1]//2, \n",
    "                image.shape[-2]//2, \n",
    "                'Loading...', \n",
    "                fontsize='x-large',\n",
    "                weight='bold',\n",
    "                va='center',\n",
    "                ha='center',\n",
    "                backgroundcolor=(1.0, 0.8, 0.8))\n",
    "load_text.set_visible(False)\n",
    "ax.axis('off')\n",
    "\n",
    "def onclick(event):\n",
    "    print('click')\n",
    "    global image, latents\n",
    "    load_text.set_visible(True)\n",
    "    fig.canvas.draw()\n",
    "    ix, iy = event.xdata, event.ydata\n",
    "    # ix = (ix / image.shape[-1] - 0.5) * 2.0\n",
    "    # iy = (iy / image.shape[-2] - 0.5) * 2.0\n",
    "    print(f'Clicked at x={ix}, y={iy}')\n",
    "    image, latents = next_step_prediction(model, image=image,\n",
    "                                            action=torch.tensor([iy, ix], \n",
    "                                                                    dtype=torch.float32,\n",
    "                                                                    device=device),\n",
    "                                            latents=latents,\n",
    "                                            plot_images=False)\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    load_text.set_visible(False)\n",
    "    fig.canvas.draw()\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = test_seq_dataset[5][0][0]\n",
    "latents = None\n",
    "action_sequence = [\n",
    "    (0.42857143, 1.),\n",
    "    (0., 0.),\n",
    "    (-0.14285714, -0.14285714),\n",
    "    (0.42857143, 1.),\n",
    "    (0., 0.),\n",
    "]\n",
    "\n",
    "for i, action in enumerate(action_sequence):\n",
    "    image, latents = next_step_prediction(model,\n",
    "                                          image=image, \n",
    "                                          action=torch.tensor(action, device=device), \n",
    "                                          latents=latents,\n",
    "                                          gt_image=test_seq_dataset[5][0][1],\n",
    "                                          plot_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "# from open_clip import get_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "# tokenizer = get_tokenizer('hf-hub:timm/ViT-B-16-SigLIP')\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "action = torch.tensor((-0.14285714, -0.14285714)).to(device)\n",
    "\n",
    "# Assuming test_seq_dataset, next_step_prediction, and device are defined elsewhere\n",
    "latents = None\n",
    "image = test_seq_dataset[0][0][0]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "load_text = ax.text(image.shape[-1]//2, \n",
    "                    image.shape[-2]//2, \n",
    "                    'Enter action and press \"Update\"', \n",
    "                    fontsize='x-large',\n",
    "                    weight='bold',\n",
    "                    va='center',\n",
    "                    ha='center',\n",
    "                    backgroundcolor=(1.0, 0.8, 0.8))\n",
    "load_text.set_visible(True)  # Initially visible to prompt for input\n",
    "ax.axis('off')\n",
    "\n",
    "# Text input for action\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type action here',\n",
    "    description='Action:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Button to trigger the update\n",
    "update_button = widgets.Button(\n",
    "    description='Update',\n",
    "    disabled=False,\n",
    "    button_style='',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to update based on action',\n",
    "    icon='check'  # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "def on_update_button_clicked(b):\n",
    "    global image, latents\n",
    "    text = text_input.value \n",
    "    \n",
    "    \n",
    "    print(f'Action: {text}') \n",
    "\n",
    "    # Update the image and latents based on the provided action\n",
    "    image, latents = next_step_prediction(model,\n",
    "                                          image=image,\n",
    "                                          action=action,  # Assuming the function now accepts a string\n",
    "                                          latents=latents,\n",
    "                                          plot_images=False,\n",
    "                                          text=text,\n",
    "                                          tokenizer=tokenizer,\n",
    "                                          text_only=True)\n",
    "                                          \n",
    "    \n",
    "    # Update the plot with the new image\n",
    "    ax.clear()  # Clear the previous image\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    load_text.set_visible(False)  # Hide the loading text\n",
    "    ax.axis('off')  # Hide axes again after redrawing\n",
    "    fig.canvas.draw()  # Refresh the figure\n",
    "\n",
    "update_button.on_click(on_update_button_clicked)\n",
    "\n",
    "# Display the widgets\n",
    "widgets.VBox([text_input, update_button])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(map(get_color_name, [eval(x.split('_')[1]) for x in causal_dicts[0].keys()]))\n",
    "# Replace the rgb values with the color names in the causal_dicts keys while keeping the rest of the key intact\n",
    "# i.e., 'obstacle_(255, 165, 0)_position_x' -> 'obstacle_orange_position_x'\n",
    "plt.imshow(frame1, origin='upper', interpolation='none')\n",
    "# Remove the axes and the white space around the image\n",
    "plt.gca().set_axis_off()\n",
    "# Remove the white space around the image\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame2, origin='upper', interpolation='none')\n",
    "# Remove the axes and the white space around the image\n",
    "plt.gca().set_axis_off()\n",
    "# Remove the white space around the image\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/miniconda3/envs/biscuit/lib/python3.11/site-packages/pytorch_lightning/utilities/migration/utils.py:55: PossibleUserWarning: The loaded checkpoint was produced with Lightning v2.1.0, which is newer than your current Lightning version: v2.0.9.post0\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://0.0.0.0:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd03b3c0710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from models.ae import Autoencoder\n",
    "\n",
    "import io\n",
    "import base64\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from dash import Dash, dcc, html, Input, Output, no_update, callback\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "import io\n",
    "import base64\n",
    "import dash\n",
    "\n",
    "# Helper functions\n",
    "def np_image_to_base64(img):\n",
    "    \"\"\"Convert a NumPy array to a base64 encoded image.\"\"\"\n",
    "    img = Image.fromarray((img * 255).astype(np.uint8))\n",
    "    buffered = io.BytesIO()\n",
    "    img.save(buffered, format=\"PNG\")\n",
    "    return f\"data:image/png;base64,{base64.b64encode(buffered.getvalue()).decode()}\"\n",
    "\n",
    "# Main function to prepare data and create Dash app\n",
    "def create_visualizations_app(model, dataloader, perplexity=30, n_neighbors=15, min_dist=0.1, max_samples=500):\n",
    "    model.eval()\n",
    "    embeddings, images, frame_positions = [], [], []\n",
    "    sample_count = 0\n",
    "\n",
    "    # Collect data\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            imgs, positions, *_ = batch  # Update this based on your data structure\n",
    "            if imgs.dim() > 4:\n",
    "                imgs = imgs.view(-1, *imgs.shape[2:])\n",
    "                positions = positions.view(-1)\n",
    "\n",
    "            if sample_count + imgs.shape[0] > max_samples:\n",
    "                limit = max_samples - sample_count\n",
    "                imgs = imgs[:limit]\n",
    "                positions = positions[:limit]\n",
    "\n",
    "            embeddings_batch = model(imgs.to('cuda')).cpu().numpy()  # Ensure device compatibility\n",
    "            embeddings.append(embeddings_batch)\n",
    "            images.extend([np_image_to_base64(np.array(img.permute(1, 2, 0))) for img in imgs])\n",
    "            frame_positions.extend(positions.cpu().numpy())\n",
    "\n",
    "            sample_count += imgs.shape[0]\n",
    "            if sample_count >= max_samples:\n",
    "                break\n",
    "\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    colors = frame_positions / np.max(frame_positions)  # Normalize for color mapping\n",
    "\n",
    "    # Perform dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity).fit_transform(embeddings)\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=2)\n",
    "    umap_result = reducer.fit_transform(embeddings)\n",
    "\n",
    "    # Create Dash app\n",
    "    app = Dash(__name__)\n",
    "\n",
    "    app.layout = html.Div([\n",
    "        dcc.Graph(id=\"graph-tsne\", figure=create_figure(tsne, images, frame_positions, 't-SNE Visualization')),\n",
    "        dcc.Graph(id=\"graph-umap\", figure=create_figure(umap_result, images, frame_positions, 'UMAP Visualization')),\n",
    "        dcc.Tooltip(id=\"graph-tooltip\")\n",
    "    ])\n",
    "\n",
    "    @callback(\n",
    "        Output(\"graph-tooltip\", \"show\"),\n",
    "        Output(\"graph-tooltip\", \"bbox\"),\n",
    "        Output(\"graph-tooltip\", \"children\"),\n",
    "        Input(\"graph-tsne\", \"hoverData\"),\n",
    "        Input(\"graph-umap\", \"hoverData\")\n",
    "    )\n",
    "    def display_hover(hoverData_tsne, hoverData_umap):\n",
    "        ctx = dash.callback_context\n",
    "\n",
    "        if not ctx.triggered:\n",
    "            return False, no_update, no_update\n",
    "\n",
    "        hover_data = ctx.triggered[0][\"value\"][\"points\"][0]\n",
    "        bbox = hover_data[\"bbox\"]\n",
    "        num = hover_data[\"pointNumber\"]\n",
    "\n",
    "        children = html.Div([\n",
    "            html.Img(src=images[num], style={\"width\": \"100px\", 'display': 'block', 'margin': '0 auto'}),\n",
    "            # html.P(f\"Frame Position: {frame_positions[num]}\", style={'font-weight': 'bold'})\n",
    "        ])\n",
    "\n",
    "        return True, bbox, children\n",
    "\n",
    "    return app\n",
    "\n",
    "def create_figure(data, images, frame_positions, title):\n",
    "    fig = go.Figure(data=[go.Scatter(\n",
    "        x=data[:, 0],\n",
    "        y=data[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(size=5, color=frame_positions, colorscale='Viridis', showscale=True),\n",
    "        text=[f'' for img, pos in zip(images, frame_positions)],\n",
    "        hoverinfo='text'\n",
    "    )])\n",
    "    fig.update_layout(title=title, xaxis_title=f'{title} Dimension 1', yaxis_title=f'{title} Dimension 2')\n",
    "    return fig\n",
    "\n",
    "model = Autoencoder.load_from_checkpoint('/home/john/PhD/BISCUIT/pretrained_models/epoch=14-step=8325.ckpt')\n",
    "from experiments.datasets import GridworldDataset\n",
    "data_folder = '/home/john/PhD/BISCUIT/data_generation/data/gridworld_simplified_2c2b2l_noturn_noshufflecars_bfix_preintv'\n",
    "train_seq_dataset = GridworldDataset(\n",
    "        data_folder=data_folder, split='val', return_targets=True, return_latents=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=1.0, return_whole_episode=True)\n",
    "train_seq_dataloader = torch.utils.data.DataLoader(train_seq_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Run the app\n",
    "app = create_visualizations_app(model, train_seq_dataloader, perplexity=30, n_neighbors=100, min_dist=0.1, max_samples=500)\n",
    "app.run_server(debug=True, port=8050, host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
