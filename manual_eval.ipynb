{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_path = '/home/gkounto/BISCUIT/llm-reasoners-pvt/examples/gridworld/models/epoch=39-step=19760.ckpt'\n",
    "model = torch.load(model_path)\n",
    "model['hyper_parameters']['autoencoder_checkpoint'] = '/home/gkounto/BISCUIT/llm-reasoners-pvt/examples/gridworld/models/AE_40l_64hid_3c1b3l.ckpt'\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.biscuit_nf import BISCUITNF\n",
    "import torch\n",
    "from typing import List, Tuple, Dict, Any, Union, Optional\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "autoencoder_path = '/home/john/PhD/BISCUIT/pretrained_models/AE_40l_64hid_3c1b3l.ckpt'\n",
    "model_path = '/home/john/PhD/BISCUIT/pretrained_models/epoch=39-step=19760.ckpt'\n",
    "model = BISCUITNF.load_from_checkpoint(model_path, autoencoder_path=autoencoder_path)\n",
    "model.to(device)\n",
    "model.freeze()\n",
    "_ = model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "encs_path = '/home/john/PhD/BISCUIT/pretrained_models/AE_gridworld_simplified/encodings/gridworld_simplified_5c_test_indep.pt'\n",
    "encs_path_drop_last_frame = '/home/john/PhD/BISCUIT/pretrained_models/AE_gridworld_simplified/encodings/gridworld_simplified_5c_drop_last_frame_test_indep.pt'\n",
    "t = torch.load(encs_path)\n",
    "t1 = torch.load(encs_path_drop_last_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "for i in range(50):\n",
    "    print(i)\n",
    "    img1 = t1[i]\n",
    "    img2 = t[i]\n",
    "    print(img1)\n",
    "    print(img2)\n",
    "    img1 = model.autoencoder.decoder(torch.from_numpy(img1).to(device).unsqueeze(0))\n",
    "    img1 = (img1 + 1) / 2\n",
    "    img2 = model.autoencoder.decoder(torch.from_numpy(img2).to(device).unsqueeze(0))\n",
    "    img2 = (img2 + 1) / 2\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(img1.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "    ax[1].imshow(img2.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_path = '/home/gkounto/BISCUIT/experiments/pretrained_models/AE_gridworld_small/encodings/gridworld_small_pre_intv_freeze_test.pt'\n",
    "encodings = torch.load(encodings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.datasets import GridworldDataset\n",
    "data_folder = '/home/john/PhD/BISCUIT/data_generation/data/gridworld_simplified_3c1b3l_noturn_noshufflecars_f'\n",
    "# val_seq_dataset = GridworldDataset(\n",
    "# data_folder=data_folder, split='val', return_targets=True, return_robot_state=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=0.01)\n",
    "train_seq_dataset = GridworldDataset(\n",
    "        data_folder=data_folder, split='val', return_targets=True, return_latents=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=True, subsample_percentage=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from experiments.datasets import GridworldDataset\n",
    "from tqdm import tqdm\n",
    "def encode_dataset(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_latents = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Wrap the dataloader with tqdm for a progress bar\n",
    "        for batch in tqdm(dataloader, desc=\"Encoding batches\"):\n",
    "            # Assuming batch[0] is the input data\n",
    "            inputs = batch[0].to(device)\n",
    "            latents = model.autoencoder.encoder(inputs)\n",
    "            disentangled_latents = model.encode(latents)\n",
    "            all_latents.append(disentangled_latents.cpu())\n",
    "\n",
    "    # Concatenate all batch outputs into a single tensor\n",
    "    all_latents = torch.cat(all_latents, dim=0)\n",
    "    return all_latents\n",
    "\n",
    "def identify_prunable_latents(latents, std_threshold=0.1):\n",
    "    # Calculate the standard deviation across the batch dimension\n",
    "    stds = torch.std(latents, dim=0)\n",
    "    print(stds)\n",
    "    prunable_indices = torch.where(stds < std_threshold)[0]\n",
    "    return prunable_indices\n",
    "\n",
    "# Parameters\n",
    "data_folder = '/home/john/PhD/BISCUIT/data/gridworld_simplified_5c'\n",
    "batch_size = 64  # Adjust batch size according to your GPU memory\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Setup dataset and dataloader\n",
    "# train_dataset = GridworldDataset(data_folder=data_folder, split='val', return_targets=True,\n",
    "#                                  return_latents=True, single_image=True, triplet=False, seq_len=2,\n",
    "#                                  cluster=False, return_text=True, subsample_percentage=1.0)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Encode all data\n",
    "model.to(device)\n",
    "all_disentangled_latents = encode_dataset(model, train_loader, device)\n",
    "\n",
    "# Identify prunable latents\n",
    "prunable_indices = identify_prunable_latents(all_disentangled_latents)\n",
    "print(f\"Latent dimensions that can be pruned: {prunable_indices.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_prunable_latents(all_disentangled_latents, std_threshold=3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from experiments.datasets import GridworldDataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer\n",
    "model_name = 'hf-hub:timm/ViT-B-16-SigLIP'\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "\n",
    "\n",
    "# Initialize the tokenizer\n",
    "# model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def process_data(model, dataloader, prune_dims, device):\n",
    "    model.eval()\n",
    "    data_tuples = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Processing data\"):\n",
    "            # Extract data components\n",
    "            image_pairs, input_ids, token_type_ids, attention_mask = batch[0], batch[-4], batch[-3], batch[-2]\n",
    "\n",
    "            # Decode text descriptions\n",
    "            action_descriptions = [\n",
    "                tokenizer.tokenizer.decode(ids[mask.bool()].tolist(), skip_special_tokens=True)\n",
    "                for ids, mask in zip(input_ids, attention_mask)\n",
    "            ]\n",
    "\n",
    "            # Process each pair of images\n",
    "            for images, action_description in zip(image_pairs, action_descriptions):\n",
    "                # Encode images to latents and then to disentangled latents\n",
    "                s0, s1 = images[0].to(device), images[1].to(device)\n",
    "                latent_s0 = model.autoencoder.encoder(s0.unsqueeze(0))\n",
    "                latent_s1 = model.autoencoder.encoder(s1.unsqueeze(0))\n",
    "                disentangled_s0 = model.encode(latent_s0).squeeze(0)\n",
    "                disentangled_s1 = model.encode(latent_s1).squeeze(0)\n",
    "\n",
    "                # Prune specific latent dimensions using the prune_dims list\n",
    "                pruned_s0 = torch.index_select(disentangled_s0, 0, torch.tensor(prune_dims, device=device))\n",
    "                pruned_s1 = torch.index_select(disentangled_s1, 0, torch.tensor(prune_dims, device=device))\n",
    "\n",
    "                # Store the tuple (pruned_s0, action_description, pruned_s1)\n",
    "                data_tuples.append((pruned_s0.cpu(), action_description, pruned_s1.cpu()))\n",
    "\n",
    "    return data_tuples\n",
    "\n",
    "# Parameters\n",
    "data_folder = '/home/john/PhD/BISCUIT/data/gridworld_simplified_5c'\n",
    "batch_size = 32  # Adjusted for processing two images per sample\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Setup dataset and dataloader\n",
    "train_dataset = GridworldDataset(data_folder=data_folder, split='train', return_targets=True,\n",
    "                                 return_latents=True, single_image=False, triplet=False, seq_len=2,\n",
    "                                 cluster=False, return_text=True, subsample_percentage=0.005)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Process data\n",
    "model.to(device)\n",
    "data_tuples = process_data(model, train_loader, list(set(range(20)) - set([12, 13, 14, 15, 16, 18])), device)\n",
    "\n",
    "# Display some samples\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"Pruned Latent s0:\", data_tuples[i][0])\n",
    "    print(\"Action Description:\", data_tuples[i][1])\n",
    "    print(\"Pruned Latent s1:\", data_tuples[i][2])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_tuples(data_tuples, num_samples):\n",
    "    sequence_strings = []\n",
    "    for i, (s0, desc, s1) in enumerate(data_tuples[:num_samples]):\n",
    "        # Format tensors with two decimal places\n",
    "        s0_formatted = ', '.join(f\"{x:.2f}\" for x in s0.numpy())\n",
    "        s1_formatted = ', '.join(f\"{x:.2f}\" for x in s1.numpy())\n",
    "\n",
    "        # Create the sequence string for the current tuple\n",
    "        sequence_string = f\"Seq{i+1}: <[{s0_formatted}], {desc}, [{s1_formatted}]>\"\n",
    "        sequence_strings.append(sequence_string)\n",
    "\n",
    "    # Combine all sequence strings into one\n",
    "    full_sequence = ' '.join(sequence_strings)\n",
    "    return full_sequence\n",
    "\n",
    "# Define the number of sequences you want to generate in the string\n",
    "num_sequences = 30  # Change this number based on your requirements\n",
    "\n",
    "# Generate formatted sequence string\n",
    "formatted_sequences = format_data_tuples(data_tuples, num_sequences)\n",
    "print(formatted_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_json_from_tuples(data_tuples, num_samples):\n",
    "    sequences = {}\n",
    "    for i, (s0, desc, s1) in enumerate(data_tuples[:num_samples], start=1):\n",
    "        # Convert tensors to formatted lists\n",
    "        s0_list = [f\"{x:.2f}\" for x in s0.numpy()]\n",
    "        s1_list = [f\"{x:.2f}\" for x in s1.numpy()]\n",
    "\n",
    "        # Create a dictionary for the current sequence\n",
    "        sequence_dict = {\n",
    "            \"s_0\": s0_list,\n",
    "            \"a\": desc,\n",
    "            \"s_1\": s1_list\n",
    "        }\n",
    "\n",
    "        # Add the dictionary to the sequences with a key indicating the sequence number\n",
    "        sequences[f\"seq{i}\"] = sequence_dict\n",
    "\n",
    "    # Convert the dictionary to JSON\n",
    "    json_output = json.dumps(sequences, indent=4)\n",
    "    return json_output\n",
    "\n",
    "# Define the number of sequences you want to convert into JSON\n",
    "num_sequences = 300  # Adjust this to include more or fewer sequences\n",
    "\n",
    "# Create JSON from the data tuples\n",
    "json_sequences = create_json_from_tuples(data_tuples, num_sequences)\n",
    "# print(json_sequences)\n",
    "# Save json\n",
    "with open('pruned_sequences.json', 'w') as f:\n",
    "    f.write(json_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from data_generation.gridworld import Gridworld\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def format_causal_dict(causal_dict):\n",
    "    formatted_text = \"\"\n",
    "    for key, value in causal_dict.items():\n",
    "        formatted_text += f\"{key}: {value}\\n\"\n",
    "        if \"position_y\" in key:  # Add a line break after each entity's details\n",
    "            formatted_text += \"\\n\"\n",
    "    return formatted_text\n",
    "\n",
    "def visualize_episode(frames, causals, causal_keys, actions, action_descriptions, interventions):\n",
    "    for i in range(len(frames)):\n",
    "        # debug_causals = Gridworld.causal_vector_to_debug_dict(causal_keys, causals[i])\n",
    "        debug_causals = dict(zip(causal_keys, causals[i]))\n",
    "        formatted_causals = format_causal_dict(debug_causals)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax1 = plt.subplot(1, 2, 1)  # Frame subplot\n",
    "        ax1.imshow(frames[i])\n",
    "        ax1.set_title(f\"Frame {i+1}\")\n",
    "        ax1.axis('off')\n",
    "\n",
    "        ax2 = plt.subplot(1, 2, 2)  # Textual information subplot\n",
    "        ax2.axis('off')\n",
    "        text_info = (\n",
    "            f\"Step {i+1}:\\n\\n\"\n",
    "            f\"Causals (Formatted):\\n{formatted_causals}\\n\"\n",
    "            f\"Action: {actions[i]}\\n\"\n",
    "            f\"Action Description: {action_descriptions[i]}\\n\"\n",
    "            f\"Interventions: {interventions[i]}\"\n",
    "        )\n",
    "        ax2.text(0, 1, text_info, ha='left', va='top', fontsize=8, wrap=True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "a = np.load('/home/john/PhD/BISCUIT/data_generation/data/gridworld_simplified_2c2b2l_noturn_noshufflecars/check/gridworld_episode_28.npz')\n",
    "causal_keys = json.load(open('/home/john/PhD/BISCUIT/data_generation/data/gridworld_simplified_2c2b2l_noturn_noshufflecars/check_metadata.json'))['flattened_causals']\n",
    "frames, causals, actions, interventions, action_descriptions = a['frames'], a['causals'], a['actions'], a['interventions'], a['action_descriptions']\n",
    "visualize_episode(frames, causals, causal_keys, actions, action_descriptions, interventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "# Assume Gridworld, format_causal_dict, and the dataset class are defined elsewhere\n",
    "\n",
    "def visualize_episodes(dataset, causal_keys, N=1):\n",
    "    for episode_idx in range(min(N, len(dataset))):  # Loop through N episodes or the total dataset length\n",
    "        frame_seq = dataset[episode_idx]  # Get the episode data\n",
    "        frames, actions, interventions, causals = frame_seq\n",
    "        \n",
    "        for i in range(frames.shape[0] - 1):  # Iterate through each step, excluding the last frame\n",
    "            # Map causals to keys for the current frame\n",
    "            debug_causals = dict(zip(causal_keys, causals[i]))\n",
    "            formatted_causals = format_causal_dict(debug_causals)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax1 = plt.subplot(1, 2, 1)  # Frame subplot\n",
    "            # Adjust image normalization if necessary\n",
    "            img = (frames[i].permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0  # Normalize if required\n",
    "            ax1.imshow(img)\n",
    "            ax1.set_title(f\"Episode {episode_idx + 1}, Frame {i + 1}\")\n",
    "            ax1.axis('off')\n",
    "\n",
    "            ax2 = plt.subplot(1, 2, 2)  # Textual information subplot\n",
    "            ax2.axis('off')\n",
    "            text_info = (\n",
    "                f\"Episode {episode_idx + 1}, Step {i + 1}:\\n\\n\"\n",
    "                f\"Causals (Formatted):\\n{formatted_causals}\\n\"\n",
    "                f\"Action: {actions[i]}\\n\"\n",
    "                f\"Interventions: {interventions[i]}\"\n",
    "            )\n",
    "            ax2.text(0, 1, text_info, ha='left', va='top', fontsize=8, wrap=True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Example usage:\n",
    "causal_keys = json.load(open('/home/gkounto/BISCUIT/data_generation/data/gridworld_simplified/train_metadata.json'))['flattened_causals']\n",
    "# Assuming train_seq_dataset is defined and loaded\n",
    "visualize_episodes(train_seq_dataset, causal_keys, N=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_img(imgs: Any,\n",
    "             figure_title: str = None,\n",
    "             titles: Optional[list] = None):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axes = plt.subplots(1, len(imgs), figsize=(len(imgs) * 3.5, 3.5))\n",
    "    if len(imgs) == 1:\n",
    "        axes = [axes]\n",
    "    for i, ax in enumerate(axes):\n",
    "        if len(imgs[i].shape) == 3 and imgs[i].shape[0] in [3,4]:\n",
    "            imgs[i] = imgs[i].permute(1, 2, 0)\n",
    "        if isinstance(imgs[i], torch.Tensor):\n",
    "            imgs[i] = imgs[i].detach().cpu().numpy()\n",
    "        ax.imshow(imgs[i])\n",
    "        ax.axis('off')\n",
    "        if titles is not None and i < len(titles):\n",
    "            ax.set_title(titles[i], weight='bold')\n",
    "    if figure_title is not None:\n",
    "        fig.suptitle(figure_title, weight='bold', size=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_exclamation_mark_image(size=256, background_color=(1, 1, 1), mark_color=(1, 0, 0), mark_thickness=10, mark_height=100, dot_radius=10):\n",
    "    \"\"\"\n",
    "    Creates an image with a red exclamation mark in the center on a white background.\n",
    "\n",
    "    Parameters:\n",
    "    - size (int): The size of the image (width and height). Default is 256.\n",
    "    - background_color (tuple): The RGB color of the background in the range 0 to 1. Default is white.\n",
    "    - mark_color (tuple): The RGB color of the exclamation mark in the range 0 to 1. Default is red.\n",
    "    - mark_thickness (int): The thickness of the line part of the exclamation mark. Default is 10.\n",
    "    - mark_height (int): The height of the line part of the exclamation mark. Default is 100.\n",
    "    - dot_radius (int): The radius of the dot part of the exclamation mark. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The generated image as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Create a size x size x 3 array with the background color\n",
    "    image = np.ones((size, size, 3)) * np.array(background_color)\n",
    "\n",
    "    # Define the center position\n",
    "    center_x, center_y = size // 2, size // 2\n",
    "\n",
    "    # Draw the line part of the exclamation mark\n",
    "    for x in range(center_x - mark_thickness // 2, center_x + mark_thickness // 2):\n",
    "        for y in range(center_y - mark_height // 2, center_y + mark_height // 2 - dot_radius * 2):\n",
    "            image[y, x] = mark_color\n",
    "\n",
    "    # Draw the dot part of the exclamation mark\n",
    "    for x in range(image.shape[1]):\n",
    "        for y in range(image.shape[0]):\n",
    "            if (x - center_x) ** 2 + (y - (center_y + mark_height // 2 + dot_radius)) ** 2 <= dot_radius ** 2:\n",
    "                image[y, x] = mark_color\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def next_step_prediction(\n",
    "        model: BISCUITNF,\n",
    "        image: torch.Tensor,\n",
    "        action: torch.Tensor,\n",
    "        gt_image: torch.Tensor = torch.zeros(1),\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        plot_images: bool = True,\n",
    "        intv_targets: Optional[torch.Tensor] = None,\n",
    "        text: Optional[str] = None,\n",
    "        tokenizer: Optional[Any] = None,\n",
    "        text_only: bool = False,\n",
    "        N: int = 8\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if latents is None:\n",
    "        input_image = (image * 2.0) - 1.0\n",
    "        latents = model.autoencoder.encoder(input_image[None])\n",
    "        latents, _ = model.flow.forward(latents)\n",
    "    if text is not None:\n",
    "        tokenized_description = tokenizer.tokenizer(text, return_token_type_ids=True, padding='max_length', max_length=64)\n",
    "        input_ids = torch.tensor(tokenized_description['input_ids']).to(device)\n",
    "        token_type_ids = torch.tensor(tokenized_description['token_type_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokenized_description['attention_mask']).to(device)\n",
    "        tokenized_description = {'input_ids': input_ids, 'token_type_ids': token_type_ids, 'attention_mask': attention_mask}    \n",
    "    new_latents, _ = model.prior_t1.sample(latents, action[None], num_samples=1, intv_targets=intv_targets, tokenized_description=tokenized_description)\n",
    "    new_latents = new_latents.squeeze(1)\n",
    "    new_encodings = model.flow.reverse(new_latents)\n",
    "    new_image = model.autoencoder.decoder(new_encodings)[0]\n",
    "    new_image = (new_image + 1.0) / 2.0\n",
    "    if plot_images:\n",
    "        gt_diff_flag = False\n",
    "        new_image_frame = new_image.permute(1, 2, 0).cpu().numpy()\n",
    "        if latents is None:\n",
    "            old_image_frame = (image.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0\n",
    "        else:\n",
    "            old_image_frame = image.permute(1, 2, 0).cpu().numpy()\n",
    "        clicked_image_frame = np.copy(old_image_frame)\n",
    "        ground_truth_image_frame = (gt_image.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0\n",
    "        difference = np.abs(new_image_frame - ground_truth_image_frame)\n",
    "        print(f\"Mean absolute difference between the new image and the ground truth: {difference.mean()}\")\n",
    "        if difference.mean() > 0.001:\n",
    "            print(\"The new image is not close to the ground truth\")\n",
    "            gt_diff_flag = True\n",
    "            exclamation_mark_image = create_exclamation_mark_image()\n",
    "        if action.ndim == 1:\n",
    "            action = action[None]\n",
    "        for i in range(action.shape[0]):\n",
    "            if torch.any(action < 0) or torch.any(action > 1):\n",
    "                continue\n",
    "            # Correct calculation for pixel positions considering the normalized action coordinates\n",
    "            pixel_x = int(action[i, 0].item() * (image.shape[-1] - 1))\n",
    "            pixel_y = int(action[i, 1].item() * (image.shape[-2] - 1))\n",
    "            # Highlight the click location with a red color\n",
    "            clicked_image_frame[max(0, pixel_y-5):pixel_y+6, \n",
    "                                max(0, pixel_x-5):pixel_x+6, \n",
    "                                :] = np.array([1.0, 1.0, 1.0])\n",
    "        if torch.any(gt_image != 0):\n",
    "            image_list = [old_image_frame, clicked_image_frame, new_image_frame, ground_truth_image_frame]\n",
    "            image_titles = ['Previous Frame', 'Click Location', 'New Sample', 'Ground Truth']\n",
    "            if gt_diff_flag:\n",
    "                image_list.append(exclamation_mark_image)\n",
    "                image_titles.append('Difference')\n",
    "            show_img(image_list,\n",
    "                     figure_title=f'Performing action {(action if action.ndim == 1 else action[0]).squeeze().cpu().numpy()}',\n",
    "                     titles=image_titles)\n",
    "        else:\n",
    "            show_img([old_image_frame, clicked_image_frame, new_image_frame],\n",
    "                    figure_title=f'Performing action {(action if action.ndim == 1 else action[0]).squeeze().cpu().numpy()}',\n",
    "                    titles=['Previous Frame', 'Click Location', 'New Sample'])\n",
    "    return new_image, new_latents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = GridworldDataset(\n",
    "\tdata_folder=data_folder, split='val', return_targets=False, single_image=True, return_latents=True, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=1.0,)\n",
    "for elem in val_dataset:\n",
    "    print(elem[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.datasets import GridworldDataset\n",
    "data_folder = '/home/john/PhD/BISCUIT/data/gridworld_simplified_5c'\n",
    "# val_seq_dataset = GridworldDataset(\n",
    "# data_folder=data_folder, split='val', return_targets=True, return_robot_state=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=0.01)\n",
    "test_seq_dataset = GridworldDataset(\n",
    "        data_folder=data_folder, split='test', return_targets=True, return_latents=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=0.01)\n",
    "val_indep_dataset = GridworldDataset(\n",
    "         data_folder=data_folder, split='val_indep', return_targets=True, single_image=True, return_latents=True, triplet=False, seq_len=1, cluster=False, return_text=False, subsample_percentage=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def reverseEnum(data: list):\n",
    "    for i in range(len(data)-1, -1, -1):\n",
    "        yield (i, data[i])\n",
    "\n",
    "for i, frame_seq in enumerate(test_seq_dataset):\n",
    "        print(i)\n",
    "        action = torch.tensor(frame_seq[1]).to(device).squeeze()\n",
    "        interventions = torch.tensor(frame_seq[2]).to(device)\n",
    "        causals = torch.tensor(frame_seq[3]).to(device)\n",
    "        print(causals)\n",
    "        print(interventions)\n",
    "        # intv_targets = torch.tensor(frame_seq[2]).to(device)\n",
    "        intv_targets = None\n",
    "        plot_images = True\n",
    "        new_image, new_latents = next_step_prediction(model, frame_seq[0][0], action, gt_image=frame_seq[0][1], plot_images=plot_images, intv_targets=intv_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, frame_seq in enumerate(val_seq_dataset):\n",
    "    # Stack the images at each step to form a sequence\n",
    "    frames, actions = frame_seq\n",
    "    frames = frames.to(device)\n",
    "    actions = torch.from_numpy(actions).to(device)\n",
    "    # interventions = interventions.to(device)\n",
    "    # causals = causals.to(device)\n",
    "    # Perform the next step prediction\n",
    "    new_image, new_latents = next_step_prediction(frames[0], actions, gt_image=frames[1], intv_targets=None, plot_images=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "latents = None\n",
    "image = test_seq_dataset[0][0][0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "load_text = ax.text(image.shape[-1]//2, \n",
    "                image.shape[-2]//2, \n",
    "                'Loading...', \n",
    "                fontsize='x-large',\n",
    "                weight='bold',\n",
    "                va='center',\n",
    "                ha='center',\n",
    "                backgroundcolor=(1.0, 0.8, 0.8))\n",
    "load_text.set_visible(False)\n",
    "ax.axis('off')\n",
    "\n",
    "def onclick(event):\n",
    "    print('click')\n",
    "    global image, latents\n",
    "    load_text.set_visible(True)\n",
    "    fig.canvas.draw()\n",
    "    ix, iy = event.xdata, event.ydata\n",
    "    # ix = (ix / image.shape[-1] - 0.5) * 2.0\n",
    "    # iy = (iy / image.shape[-2] - 0.5) * 2.0\n",
    "    print(f'Clicked at x={ix}, y={iy}')\n",
    "    image, latents = next_step_prediction(model, image=image,\n",
    "                                            action=torch.tensor([iy, ix], \n",
    "                                                                    dtype=torch.float32,\n",
    "                                                                    device=device),\n",
    "                                            latents=latents,\n",
    "                                            plot_images=False)\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    load_text.set_visible(False)\n",
    "    fig.canvas.draw()\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = test_seq_dataset[5][0][0]\n",
    "latents = None\n",
    "action_sequence = [\n",
    "    (0.42857143, 1.),\n",
    "    (0., 0.),\n",
    "    (-0.14285714, -0.14285714),\n",
    "    (0.42857143, 1.),\n",
    "    (0., 0.),\n",
    "]\n",
    "\n",
    "for i, action in enumerate(action_sequence):\n",
    "    image, latents = next_step_prediction(model,\n",
    "                                          image=image, \n",
    "                                          action=torch.tensor(action, device=device), \n",
    "                                          latents=latents,\n",
    "                                          gt_image=test_seq_dataset[5][0][1],\n",
    "                                          plot_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "# Assume Gridworld, format_causal_dict, and the dataset class are defined elsewhere\n",
    "\n",
    "def visualize_episodes(dataset, causal_keys, N=1):\n",
    "    for episode_idx in range(min(N, len(dataset))):  # Loop through N episodes or the total dataset length\n",
    "        frame_seq = dataset[episode_idx]  # Get the episode data\n",
    "        frames, actions, interventions, causals = frame_seq\n",
    "        \n",
    "        for i in range(frames.shape[0] - 1):  # Iterate through each step, excluding the last frame\n",
    "            # Map causals to keys for the current frame\n",
    "            debug_causals = dict(zip(causal_keys, causals[i]))\n",
    "            print(debug_causals)\n",
    "            formatted_causals = format_causal_dict(debug_causals)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax1 = plt.subplot(1, 2, 1)  # Frame subplot\n",
    "            # Adjust image normalization if necessary\n",
    "            img = (frames[i].permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0  # Normalize if required\n",
    "            ax1.imshow(img)\n",
    "            ax1.set_title(f\"Episode {episode_idx + 1}, Frame {i + 1}\")\n",
    "            ax1.axis('off')\n",
    "\n",
    "            ax2 = plt.subplot(1, 2, 2)  # Textual information subplot\n",
    "            ax2.axis('off')\n",
    "            text_info = (\n",
    "                f\"Episode {episode_idx + 1}, Step {i + 1}:\\n\\n\"\n",
    "                f\"Causals (Formatted):\\n{formatted_causals}\\n\"\n",
    "                f\"Action: {actions[i]}\\n\"\n",
    "                f\"Interventions: {interventions[i]}\"\n",
    "            )\n",
    "            ax2.text(0, 1, text_info, ha='left', va='top', fontsize=8, wrap=True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Example usage:\n",
    "causal_keys = json.load(open('/home/john/PhD/BISCUIT/data/gridworld_simplified_5c/train_metadata.json'))['flattened_causals']\n",
    "# Assuming train_seq_dataset is defined and loaded\n",
    "visualize_episodes(train_seq_dataset, causal_keys, N=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_img(imgs: Any,\n",
    "             figure_title: str = None,\n",
    "             titles: Optional[list] = None):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axes = plt.subplots(1, len(imgs), figsize=(len(imgs) * 3.5, 3.5))\n",
    "    if len(imgs) == 1:\n",
    "        axes = [axes]\n",
    "    for i, ax in enumerate(axes):\n",
    "        if len(imgs[i].shape) == 3 and imgs[i].shape[0] in [3,4]:\n",
    "            imgs[i] = imgs[i].permute(1, 2, 0)\n",
    "        if isinstance(imgs[i], torch.Tensor):\n",
    "            imgs[i] = imgs[i].detach().cpu().numpy()\n",
    "        ax.imshow(imgs[i])\n",
    "        ax.axis('off')\n",
    "        if titles is not None and i < len(titles):\n",
    "            ax.set_title(titles[i], weight='bold')\n",
    "    if figure_title is not None:\n",
    "        fig.suptitle(figure_title, weight='bold', size=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_exclamation_mark_image(size=256, background_color=(1, 1, 1), mark_color=(1, 0, 0), mark_thickness=10, mark_height=100, dot_radius=10):\n",
    "    \"\"\"\n",
    "    Creates an image with a red exclamation mark in the center on a white background.\n",
    "\n",
    "    Parameters:\n",
    "    - size (int): The size of the image (width and height). Default is 256.\n",
    "    - background_color (tuple): The RGB color of the background in the range 0 to 1. Default is white.\n",
    "    - mark_color (tuple): The RGB color of the exclamation mark in the range 0 to 1. Default is red.\n",
    "    - mark_thickness (int): The thickness of the line part of the exclamation mark. Default is 10.\n",
    "    - mark_height (int): The height of the line part of the exclamation mark. Default is 100.\n",
    "    - dot_radius (int): The radius of the dot part of the exclamation mark. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The generated image as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Create a size x size x 3 array with the background color\n",
    "    image = np.ones((size, size, 3)) * np.array(background_color)\n",
    "\n",
    "    # Define the center position\n",
    "    center_x, center_y = size // 2, size // 2\n",
    "\n",
    "    # Draw the line part of the exclamation mark\n",
    "    for x in range(center_x - mark_thickness // 2, center_x + mark_thickness // 2):\n",
    "        for y in range(center_y - mark_height // 2, center_y + mark_height // 2 - dot_radius * 2):\n",
    "            image[y, x] = mark_color\n",
    "\n",
    "    # Draw the dot part of the exclamation mark\n",
    "    for x in range(image.shape[1]):\n",
    "        for y in range(image.shape[0]):\n",
    "            if (x - center_x) ** 2 + (y - (center_y + mark_height // 2 + dot_radius)) ** 2 <= dot_radius ** 2:\n",
    "                image[y, x] = mark_color\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def next_step_prediction(\n",
    "        model: BISCUITNF,\n",
    "        image: torch.Tensor,\n",
    "        action: torch.Tensor,\n",
    "        gt_image: torch.Tensor = torch.zeros(1),\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        plot_images: bool = True,\n",
    "        intv_targets: Optional[torch.Tensor] = None,\n",
    "        text: Optional[str] = None,\n",
    "        tokenizer: Optional[Any] = None,\n",
    "        text_only: bool = False,\n",
    "        tokenized_description=None,\n",
    "        N: int = 8\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if latents is None:\n",
    "        input_image = (image * 2.0) - 1.0\n",
    "        latents = model.autoencoder.encoder(input_image[None])\n",
    "        latents, _ = model.flow.forward(latents)\n",
    "    # tokenized_description = None\n",
    "    if text is not None:\n",
    "        tokenized_description = tokenizer(text, return_token_type_ids=True, padding='max_length', max_length=64)\n",
    "        input_ids = torch.tensor(tokenized_description['input_ids']).to(device)\n",
    "        token_type_ids = torch.tensor(tokenized_description['token_type_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokenized_description['attention_mask']).to(device)\n",
    "        tokenized_description = {'input_ids': input_ids, 'token_type_ids': token_type_ids, 'attention_mask': attention_mask}    \n",
    "    new_latents, _ = model.prior_t1.sample(latents, action[None], num_samples=1, intv_targets=intv_targets, tokenized_description=tokenized_description)\n",
    "    new_latents = new_latents.squeeze(1)\n",
    "    new_encodings = model.flow.reverse(new_latents)\n",
    "    new_image = model.autoencoder.decoder(new_encodings)[0]\n",
    "    new_image = (new_image + 1.0) / 2.0\n",
    "    if plot_images:\n",
    "        gt_diff_flag = False\n",
    "        new_image_frame = new_image.permute(1, 2, 0).cpu().numpy()\n",
    "        if latents is None:\n",
    "            old_image_frame = (image.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0\n",
    "        else:\n",
    "            old_image_frame = image.permute(1, 2, 0).cpu().numpy()\n",
    "        clicked_image_frame = np.copy(old_image_frame)\n",
    "        ground_truth_image_frame = (gt_image.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0\n",
    "        difference = np.abs(new_image_frame - ground_truth_image_frame)\n",
    "        print(f\"Mean absolute difference between the new image and the ground truth: {difference.mean()}\")\n",
    "        if difference.mean() > 0.001:\n",
    "            print(\"The new image is not close to the ground truth\")\n",
    "            gt_diff_flag = True\n",
    "            exclamation_mark_image = create_exclamation_mark_image()\n",
    "        if action.ndim == 1:\n",
    "            action = action[None]\n",
    "        for i in range(action.shape[0]):\n",
    "            if torch.any(action < 0) or torch.any(action > 1):\n",
    "                continue\n",
    "            # Correct calculation for pixel positions considering the normalized action coordinates\n",
    "            pixel_x = int(action[i, 0].item() * (image.shape[-1] - 1))\n",
    "            pixel_y = int(action[i, 1].item() * (image.shape[-2] - 1))\n",
    "            # Highlight the click location with a red color\n",
    "            clicked_image_frame[max(0, pixel_y-5):pixel_y+6, \n",
    "                                max(0, pixel_x-5):pixel_x+6, \n",
    "                                :] = np.array([1.0, 1.0, 1.0])\n",
    "        if torch.any(gt_image != 0):\n",
    "            image_list = [old_image_frame, clicked_image_frame, new_image_frame, ground_truth_image_frame]\n",
    "            image_titles = ['Previous Frame', 'Click Location', 'New Sample', 'Ground Truth']\n",
    "            if gt_diff_flag:\n",
    "                image_list.append(exclamation_mark_image)\n",
    "                image_titles.append('Difference')\n",
    "            show_img(image_list,\n",
    "                     figure_title=f'Performing action {(action if action.ndim == 1 else action[0]).squeeze().cpu().numpy()}',\n",
    "                     titles=image_titles)\n",
    "        else:\n",
    "            show_img([old_image_frame, clicked_image_frame, new_image_frame],\n",
    "                    figure_title=f'Performing action {(action if action.ndim == 1 else action[0]).squeeze().cpu().numpy()}',\n",
    "                    titles=['Previous Frame', 'Click Location', 'New Sample'])\n",
    "    return new_image, new_latents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = GridworldDataset(\n",
    "\tdata_folder=data_folder, split='val_indep', return_targets=False, single_image=True, return_latents=True, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=1.0,)\n",
    "for elem in val_dataset:\n",
    "    print(elem[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.datasets import GridworldDataset\n",
    "data_folder = '/home/john/PhD/BISCUIT/data_generation/data/gridworld_simplified_3c1b3l_noturn_noshufflecars_f'\n",
    "# val_seq_dataset = GridworldDataset(\n",
    "# data_folder=data_folder, split='val', return_targets=True, return_robot_state=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=0.01)\n",
    "test_seq_dataset = GridworldDataset(\n",
    "        data_folder=data_folder, split='val', return_targets=True, return_latents=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=True, subsample_percentage=0.01)\n",
    "# val_indep_dataset = GridworldDataset(\n",
    "#          data_folder=data_folder, split='val_indep', return_targets=True, single_image=True, return_latents=True, triplet=False, seq_len=1, cluster=False, return_text=True, subsample_percentage=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "test_elem = test_seq_dataset[0]\n",
    "input_ids, token_type_ids, attention_mask = test_elem[-4:-1]\n",
    "tokenizer.decode(input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def reverseEnum(data: list):\n",
    "    for i in range(len(data)-1, -1, -1):\n",
    "        yield (i, data[i])\n",
    "\n",
    "for i, frame_seq in enumerate(test_seq_dataset):\n",
    "        print(i)\n",
    "        action = torch.tensor(frame_seq[1]).to(device).squeeze()\n",
    "        interventions = torch.tensor(frame_seq[2]).to(device)\n",
    "        causals = torch.tensor(frame_seq[3]).to(device)\n",
    "        tokenized_description = {'input_ids': torch.tensor(frame_seq[3]).to(device), 'token_type_ids': torch.tensor(frame_seq[4]).to(device), 'attention_mask': torch.tensor(frame_seq[5]).to(device)}\n",
    "        print(causals)\n",
    "        print(interventions)\n",
    "        # intv_targets = torch.tensor(frame_seq[2]).to(device)\n",
    "        intv_targets = None\n",
    "        plot_images = True\n",
    "        new_image, new_latents = next_step_prediction(model, frame_seq[0][0], action, gt_image=frame_seq[0][1], plot_images=plot_images, intv_targets=intv_targets, tokenized_description=tokenized_description, text_only=False, N=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq_dataset[94][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "frame1 = test_seq_dataset[idx][0][0]\n",
    "frame2 = test_seq_dataset[idx][0][1]\n",
    "tokenized_description = {'input_ids': torch.tensor(test_seq_dataset[idx][3]).to(device), 'token_type_ids': torch.tensor(test_seq_dataset[idx][4]).to(device), 'attention_mask': torch.tensor(test_seq_dataset[idx][5]).to(device)}\n",
    "zdt = model.flow(model.autoencoder.encoder(frame1[None]))\n",
    "zdtplus1 = model.flow(model.autoencoder.encoder(frame2[None]))\n",
    "zdtplus1prior = model.prior_t1.sample(zdt[0], action=torch.tensor(test_seq_dataset[94][1]).to(device), num_samples=1, intv_targets=None, tokenized_description=tokenized_description)\n",
    "print(zdt)\n",
    "print(zdtplus1)\n",
    "zdtplus1flow = model.flow.reverse(zdtplus1[0])\n",
    "zdtplus1flow = model.autoencoder.decoder(zdtplus1flow)\n",
    "zdtplus1flow = (zdtplus1flow + 1) / 2\n",
    "zdtplus1prior_ = model.flow.reverse(zdtplus1prior[0])\n",
    "zdtplus1prior_ = model.autoencoder.decoder(zdtplus1prior_)\n",
    "zdtplus1prior_ = (zdtplus1prior_ + 1) / 2\n",
    "from matplotlib import pyplot as plt\n",
    "# plt.imshow(zt.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "# plt.imshow(zdtplus1prior_.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "plt.imshow(zdtplus1flow.squeeze().cpu().detach().numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zdtplus1prior[0] - zdtplus1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, frame_seq in enumerate(val_seq_dataset):\n",
    "    # Stack the images at each step to form a sequence\n",
    "    frames, actions = frame_seq\n",
    "    frames = frames.to(device)\n",
    "    actions = torch.from_numpy(actions).to(device)\n",
    "    # interventions = interventions.to(device)\n",
    "    # causals = causals.to(device)\n",
    "    # Perform the next step prediction\n",
    "    new_image, new_latents = next_step_prediction(frames[0], actions, gt_image=frames[1], intv_targets=None, plot_images=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "latents = None\n",
    "image = test_seq_dataset[0][0][0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "load_text = ax.text(image.shape[-1]//2, \n",
    "                image.shape[-2]//2, \n",
    "                'Loading...', \n",
    "                fontsize='x-large',\n",
    "                weight='bold',\n",
    "                va='center',\n",
    "                ha='center',\n",
    "                backgroundcolor=(1.0, 0.8, 0.8))\n",
    "load_text.set_visible(False)\n",
    "ax.axis('off')\n",
    "\n",
    "def onclick(event):\n",
    "    print('click')\n",
    "    global image, latents\n",
    "    load_text.set_visible(True)\n",
    "    fig.canvas.draw()\n",
    "    ix, iy = event.xdata, event.ydata\n",
    "    # ix = (ix / image.shape[-1] - 0.5) * 2.0\n",
    "    # iy = (iy / image.shape[-2] - 0.5) * 2.0\n",
    "    print(f'Clicked at x={ix}, y={iy}')\n",
    "    image, latents = next_step_prediction(model, image=image,\n",
    "                                            action=torch.tensor([iy, ix], \n",
    "                                                                    dtype=torch.float32,\n",
    "                                                                    device=device),\n",
    "                                            latents=latents,\n",
    "                                            plot_images=False)\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    load_text.set_visible(False)\n",
    "    fig.canvas.draw()\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = test_seq_dataset[5][0][0]\n",
    "latents = None\n",
    "action_sequence = [\n",
    "    (0.42857143, 1.),\n",
    "    (0., 0.),\n",
    "    (-0.14285714, -0.14285714),\n",
    "    (0.42857143, 1.),\n",
    "    (0., 0.),\n",
    "]\n",
    "\n",
    "for i, action in enumerate(action_sequence):\n",
    "    image, latents = next_step_prediction(model,\n",
    "                                          image=image, \n",
    "                                          action=torch.tensor(action, device=device), \n",
    "                                          latents=latents,\n",
    "                                          gt_image=test_seq_dataset[5][0][1],\n",
    "                                          plot_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "# from open_clip import get_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "# tokenizer = get_tokenizer('hf-hub:timm/ViT-B-16-SigLIP')\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "action = torch.tensor((-0.14285714, -0.14285714)).to(device)\n",
    "\n",
    "# Assuming test_seq_dataset, next_step_prediction, and device are defined elsewhere\n",
    "latents = None\n",
    "image = test_seq_dataset[0][0][0]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "load_text = ax.text(image.shape[-1]//2, \n",
    "                    image.shape[-2]//2, \n",
    "                    'Enter action and press \"Update\"', \n",
    "                    fontsize='x-large',\n",
    "                    weight='bold',\n",
    "                    va='center',\n",
    "                    ha='center',\n",
    "                    backgroundcolor=(1.0, 0.8, 0.8))\n",
    "load_text.set_visible(True)  # Initially visible to prompt for input\n",
    "ax.axis('off')\n",
    "\n",
    "# Text input for action\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type action here',\n",
    "    description='Action:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Button to trigger the update\n",
    "update_button = widgets.Button(\n",
    "    description='Update',\n",
    "    disabled=False,\n",
    "    button_style='',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to update based on action',\n",
    "    icon='check'  # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "def on_update_button_clicked(b):\n",
    "    global image, latents\n",
    "    text = text_input.value \n",
    "    \n",
    "    \n",
    "    print(f'Action: {text}') \n",
    "\n",
    "    # Update the image and latents based on the provided action\n",
    "    image, latents = next_step_prediction(model,\n",
    "                                          image=image,\n",
    "                                          action=action,  # Assuming the function now accepts a string\n",
    "                                          latents=latents,\n",
    "                                          plot_images=False,\n",
    "                                          text=text,\n",
    "                                          tokenizer=tokenizer,\n",
    "                                          text_only=True)\n",
    "                                          \n",
    "    \n",
    "    # Update the plot with the new image\n",
    "    ax.clear()  # Clear the previous image\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    load_text.set_visible(False)  # Hide the loading text\n",
    "    ax.axis('off')  # Hide axes again after redrawing\n",
    "    fig.canvas.draw()  # Refresh the figure\n",
    "\n",
    "update_button.on_click(on_update_button_clicked)\n",
    "\n",
    "# Display the widgets\n",
    "widgets.VBox([text_input, update_button])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(map(get_color_name, [eval(x.split('_')[1]) for x in causal_dicts[0].keys()]))\n",
    "# Replace the rgb values with the color names in the causal_dicts keys while keeping the rest of the key intact\n",
    "# i.e., 'obstacle_(255, 165, 0)_position_x' -> 'obstacle_orange_position_x'\n",
    "plt.imshow(frame1, origin='upper', interpolation='none')\n",
    "# Remove the axes and the white space around the image\n",
    "plt.gca().set_axis_off()\n",
    "# Remove the white space around the image\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame2, origin='upper', interpolation='none')\n",
    "# Remove the axes and the white space around the image\n",
    "plt.gca().set_axis_off()\n",
    "# Remove the white space around the image\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from models.ae import Autoencoder\n",
    "\n",
    "import io\n",
    "import base64\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from dash import Dash, dcc, html, Input, Output, no_update, callback\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "import io\n",
    "import base64\n",
    "import dash\n",
    "\n",
    "# Helper functions\n",
    "def np_image_to_base64(img):\n",
    "    \"\"\"Convert a NumPy array to a base64 encoded image.\"\"\"\n",
    "    img = Image.fromarray((img * 255).astype(np.uint8))\n",
    "    buffered = io.BytesIO()\n",
    "    img.save(buffered, format=\"PNG\")\n",
    "    return f\"data:image/png;base64,{base64.b64encode(buffered.getvalue()).decode()}\"\n",
    "\n",
    "# Main function to prepare data and create Dash app\n",
    "def create_visualizations_app(model, dataloader, perplexity=30, n_neighbors=15, min_dist=0.1, max_samples=500):\n",
    "    model.eval()\n",
    "    embeddings, images, frame_positions = [], [], []\n",
    "    sample_count = 0\n",
    "\n",
    "    # Collect data\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            imgs, positions, *_ = batch  # Update this based on your data structure\n",
    "            if imgs.dim() > 4:\n",
    "                imgs = imgs.view(-1, *imgs.shape[2:])\n",
    "                positions = positions.view(-1)\n",
    "\n",
    "            if sample_count + imgs.shape[0] > max_samples:\n",
    "                limit = max_samples - sample_count\n",
    "                imgs = imgs[:limit]\n",
    "                positions = positions[:limit]\n",
    "\n",
    "            embeddings_batch = model(imgs.to('cuda')).cpu().numpy()  # Ensure device compatibility\n",
    "            embeddings.append(embeddings_batch)\n",
    "            images.extend([np_image_to_base64(np.array(img.permute(1, 2, 0))) for img in imgs])\n",
    "            frame_positions.extend(positions.cpu().numpy())\n",
    "\n",
    "            sample_count += imgs.shape[0]\n",
    "            if sample_count >= max_samples:\n",
    "                break\n",
    "\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    colors = frame_positions / np.max(frame_positions)  # Normalize for color mapping\n",
    "\n",
    "    # Perform dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity).fit_transform(embeddings)\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=2)\n",
    "    umap_result = reducer.fit_transform(embeddings)\n",
    "\n",
    "    # Create Dash app\n",
    "    app = Dash(__name__)\n",
    "\n",
    "    app.layout = html.Div([\n",
    "        dcc.Graph(id=\"graph-tsne\", figure=create_figure(tsne, images, frame_positions, 't-SNE Visualization')),\n",
    "        dcc.Graph(id=\"graph-umap\", figure=create_figure(umap_result, images, frame_positions, 'UMAP Visualization')),\n",
    "        dcc.Tooltip(id=\"graph-tooltip\")\n",
    "    ])\n",
    "\n",
    "    @callback(\n",
    "        Output(\"graph-tooltip\", \"show\"),\n",
    "        Output(\"graph-tooltip\", \"bbox\"),\n",
    "        Output(\"graph-tooltip\", \"children\"),\n",
    "        Input(\"graph-tsne\", \"hoverData\"),\n",
    "        Input(\"graph-umap\", \"hoverData\")\n",
    "    )\n",
    "    def display_hover(hoverData_tsne, hoverData_umap):\n",
    "        ctx = dash.callback_context\n",
    "\n",
    "        if not ctx.triggered:\n",
    "            return False, no_update, no_update\n",
    "\n",
    "        hover_data = ctx.triggered[0][\"value\"][\"points\"][0]\n",
    "        bbox = hover_data[\"bbox\"]\n",
    "        num = hover_data[\"pointNumber\"]\n",
    "\n",
    "        children = html.Div([\n",
    "            html.Img(src=images[num], style={\"width\": \"100px\", 'display': 'block', 'margin': '0 auto'}),\n",
    "            # html.P(f\"Frame Position: {frame_positions[num]}\", style={'font-weight': 'bold'})\n",
    "        ])\n",
    "\n",
    "        return True, bbox, children\n",
    "\n",
    "    return app\n",
    "\n",
    "def create_figure(data, images, frame_positions, title):\n",
    "    fig = go.Figure(data=[go.Scatter(\n",
    "        x=data[:, 0],\n",
    "        y=data[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(size=5, color=frame_positions, colorscale='Viridis', showscale=True),\n",
    "        text=[f'' for img, pos in zip(images, frame_positions)],\n",
    "        hoverinfo='text'\n",
    "    )])\n",
    "    fig.update_layout(title=title, xaxis_title=f'{title} Dimension 1', yaxis_title=f'{title} Dimension 2')\n",
    "    return fig\n",
    "\n",
    "model = Autoencoder.load_from_checkpoint('/home/john/PhD/BISCUIT/pretrained_models/epoch=14-step=8325.ckpt')\n",
    "from experiments.datasets import GridworldDataset\n",
    "data_folder = '/home/john/PhD/BISCUIT/data_generation/data/gridworld_simplified_2c2b2l_noturn_noshufflecars_bfix_preintv'\n",
    "train_seq_dataset = GridworldDataset(\n",
    "        data_folder=data_folder, split='val', return_targets=True, return_latents=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=1.0, return_whole_episode=True)\n",
    "train_seq_dataloader = torch.utils.data.DataLoader(train_seq_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Run the app\n",
    "app = create_visualizations_app(model, train_seq_dataloader, perplexity=30, n_neighbors=100, min_dist=0.1, max_samples=300)\n",
    "app.run_server(debug=True, port=8050, host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_goals(example, return_raw=False):\n",
    "    \"\"\"Extract the goals from the example.\n",
    "    \n",
    "    :param example: dict with 'question' key containing goal and plan statements\n",
    "    :param return_raw: if True, returns the raw goal statement\n",
    "    :return: Either raw goal string or parsed goals as a dictionary\n",
    "    \"\"\"\n",
    "    goal_statement = example[\"question\"].split(\"[STATEMENT]\")[-1]\\\n",
    "        .split(\"My goal is to \")[1].split(\"My plan is as follows\")[0].strip()\n",
    "    \n",
    "    if return_raw:\n",
    "        return goal_statement\n",
    "    \n",
    "    # Improved regular expression to extract goal elements (entity, attribute, value)\n",
    "    pattern = r\"(\\w+)\\s(\\w+\\s\\w+)\\sis\\s([\\d\\.]+)(?=\\,|\\s|$)\"\n",
    "    goals = {match.group(1) + ' ' + match.group(2): float(match.group(3).rstrip('.')) for match in re.finditer(pattern, goal_statement)}\n",
    "    \n",
    "    return goals\n",
    "\n",
    "def goal_check(goals, description, epsilon=0.07, ignore_obstacles=False):\n",
    "    \"\"\"Check if the description matches the goals with a tolerance and return the percentage of goals met.\n",
    "    \n",
    "    :param goals: dictionary of goal states\n",
    "    :param description: description string containing current states\n",
    "    :param epsilon: tolerance for numeric comparisons\n",
    "    :param ignore_obstacles: if True, obstacle positions are ignored in the check\n",
    "    :return: Tuple (boolean, float) where boolean is True if all goals are met within tolerance, and float is the percentage of goals met\n",
    "    \"\"\"\n",
    "    # Parse the description into a dictionary\n",
    "    pattern = r\"(\\w+)\\s(\\w+\\s\\w+)\\sis\\s([\\d\\.]+)(?=\\,|\\s|$)\"\n",
    "    current_states = {match.group(1) + ' ' + match.group(2): float(match.group(3).rstrip('.')) for match in re.finditer(pattern, description)}\n",
    "    \n",
    "    total_goals = 0\n",
    "    met_goals = 0\n",
    "    for key, goal_value in goals.items():\n",
    "        if ignore_obstacles and 'obstacle' in key:\n",
    "            continue\n",
    "        total_goals += 1\n",
    "        if key in current_states and (abs(current_states[key] - goal_value) <= epsilon):\n",
    "            met_goals += 1\n",
    "\n",
    "    all_goals_met = met_goals == total_goals\n",
    "    percentage_met = (met_goals / total_goals) * 100 if total_goals > 0 else 0\n",
    "    return all_goals_met, percentage_met\n",
    "\n",
    "# Testing\n",
    "example = {\n",
    "    \"question\": \"[STATEMENT] This is the context. My goal is to orange obstacle position x is 1.0, orange obstacle position y is 0.86, cyan trafficlight state is 0.00, olive trafficlight state is 1.00, silver trafficlight state is 0.00, blue vehicle position y is 0.86, silver vehicle position y is 0.71, red vehicle position y is 0.14. My plan is as follows: execute movement.\"\n",
    "}\n",
    "description = \"orange obstacle position x is 0.57, orange obstacle position y is 0.86, cyan trafficlight state is 0.00, olive trafficlight state is 1.00, silver trafficlight state is 0.00, blue vehicle position y is 0.86, silver vehicle position y is 0.71, red vehicle position y is 0.14.\"\n",
    "\n",
    "goals = extract_goals(example)\n",
    "result = goal_check(goals, description, ignore_obstacles=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
