{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_path)\n",
    "model['hyper_parameters']['autoencoder_checkpoint'] = '/home/john/PhD/BISCUIT/pretrained_models/AE_gridworld_simplified/AE_40l_64hid.ckpt'\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/miniconda3/envs/biscuit/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/home/john/miniconda3/envs/biscuit/lib/python3.11/site-packages/pytorch_lightning/utilities/migration/utils.py:55: PossibleUserWarning: The loaded checkpoint was produced with Lightning v2.1.0, which is newer than your current Lightning version: v2.0.9.post0\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from models.biscuit_nf import BISCUITNF\n",
    "import torch\n",
    "from typing import List, Tuple, Dict, Any, Union, Optional\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "autoencoder_path = '/home/john/PhD/BISCUIT/pretrained_models/AE_40l_64hid_3c1b3l.ckpt'\n",
    "model_path = '/home/john/PhD/BISCUIT/pretrained_models/epoch=39-step=19760.ckpt'\n",
    "model = BISCUITNF.load_from_checkpoint(model_path, autoencoder_path=autoencoder_path)\n",
    "model.to(device)\n",
    "model.freeze()\n",
    "_ = model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "encs_path = '/home/john/PhD/BISCUIT/pretrained_models/AE_gridworld_simplified/encodings/gridworld_simplified_5c_test_indep.pt'\n",
    "encs_path_drop_last_frame = '/home/john/PhD/BISCUIT/pretrained_models/AE_gridworld_simplified/encodings/gridworld_simplified_5c_drop_last_frame_test_indep.pt'\n",
    "t = torch.load(encs_path)\n",
    "t1 = torch.load(encs_path_drop_last_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "for i in range(50):\n",
    "    print(i)\n",
    "    img1 = t1[i]\n",
    "    img2 = t[i]\n",
    "    print(img1)\n",
    "    print(img2)\n",
    "    img1 = model.autoencoder.decoder(torch.from_numpy(img1).to(device).unsqueeze(0))\n",
    "    img1 = (img1 + 1) / 2\n",
    "    img2 = model.autoencoder.decoder(torch.from_numpy(img2).to(device).unsqueeze(0))\n",
    "    img2 = (img2 + 1) / 2\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(img1.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "    ax[1].imshow(img2.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_path = '/home/gkounto/BISCUIT/experiments/pretrained_models/AE_gridworld_small/encodings/gridworld_small_pre_intv_freeze_test.pt'\n",
    "encodings = torch.load(encodings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sequences of val:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    }
   ],
   "source": [
    "from experiments.datasets import GridworldDataset\n",
    "data_folder = '/home/john/PhD/BISCUIT/data_generation/data/gridworld_simplified_3c1b3l_noturn_noshufflecars_f'\n",
    "# val_seq_dataset = GridworldDataset(\n",
    "# data_folder=data_folder, split='val', return_targets=True, return_robot_state=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=0.01)\n",
    "train_seq_dataset = GridworldDataset(\n",
    "        data_folder=data_folder, split='val', return_targets=True, return_latents=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=True, subsample_percentage=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.3245, 3.7136, 4.1374, 4.3170, 4.1890, 3.9721, 3.5552, 4.6210, 3.5418,\n",
      "        5.3126, 3.4409, 5.0134, 1.9434, 2.2111, 1.3638, 1.3471, 1.8532, 6.8994,\n",
      "        1.6336, 7.0921])\n",
      "Latent dimensions that can be pruned: []\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from experiments.datasets import GridworldDataset\n",
    "from tqdm import tqdm\n",
    "def encode_dataset(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_latents = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Wrap the dataloader with tqdm for a progress bar\n",
    "        for batch in tqdm(dataloader, desc=\"Encoding batches\"):\n",
    "            # Assuming batch[0] is the input data\n",
    "            inputs = batch[0].to(device)\n",
    "            latents = model.autoencoder.encoder(inputs)\n",
    "            disentangled_latents = model.encode(latents)\n",
    "            all_latents.append(disentangled_latents.cpu())\n",
    "\n",
    "    # Concatenate all batch outputs into a single tensor\n",
    "    all_latents = torch.cat(all_latents, dim=0)\n",
    "    return all_latents\n",
    "\n",
    "def identify_prunable_latents(latents, std_threshold=0.1):\n",
    "    # Calculate the standard deviation across the batch dimension\n",
    "    stds = torch.std(latents, dim=0)\n",
    "    print(stds)\n",
    "    prunable_indices = torch.where(stds < std_threshold)[0]\n",
    "    return prunable_indices\n",
    "\n",
    "# Parameters\n",
    "data_folder = '/home/john/PhD/BISCUIT/data/gridworld_simplified_5c'\n",
    "batch_size = 64  # Adjust batch size according to your GPU memory\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Setup dataset and dataloader\n",
    "# train_dataset = GridworldDataset(data_folder=data_folder, split='val', return_targets=True,\n",
    "#                                  return_latents=True, single_image=True, triplet=False, seq_len=2,\n",
    "#                                  cluster=False, return_text=True, subsample_percentage=1.0)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Encode all data\n",
    "model.to(device)\n",
    "all_disentangled_latents = encode_dataset(model, train_loader, device)\n",
    "\n",
    "# Identify prunable latents\n",
    "prunable_indices = identify_prunable_latents(all_disentangled_latents)\n",
    "print(f\"Latent dimensions that can be pruned: {prunable_indices.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.3245, 3.7136, 4.1374, 4.3170, 4.1890, 3.9721, 3.5552, 4.6210, 3.5418,\n",
      "        5.3126, 3.4409, 5.0134, 1.9434, 2.2111, 1.3638, 1.3471, 1.8532, 6.8994,\n",
      "        1.6336, 7.0921])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([12, 13, 14, 15, 16, 18])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identify_prunable_latents(all_disentangled_latents, std_threshold=3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 30/30 [00:35<00:00,  1.18s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Pruned Latent s0: tensor([-5.1105, -0.0313,  2.5049,  4.8884,  3.1617, -0.0855, -2.1020,  9.1485,\n",
      "        -4.7292, -3.7658,  4.6165, -9.4115,  4.8306,  2.4634])\n",
      "Action Description: You skillfully changed the state of the sturdy, illuminated, silver traffic light.\n",
      "Pruned Latent s1: tensor([ -5.0844,  -0.5309,   2.9766,  -3.4161,   4.3827,   0.3204,  -0.3343,\n",
      "          8.6096,  -4.4251,  -5.7085,   4.1223, -10.9092,   4.7426,   4.8466])\n",
      "\n",
      "Sample 2:\n",
      "Pruned Latent s0: tensor([ -4.5394,  -0.1422,   3.6258,  -4.3175,   3.7612,   0.3745,  -1.8695,\n",
      "          9.6974,  -4.9644,  -4.1669,   3.7390, -12.9657,   4.2706,   4.8516])\n",
      "Action Description: You efficiently changed the state of the metal, tall, cyan traffic light.\n",
      "Pruned Latent s1: tensor([-5.1930, -0.3977,  1.6693, -4.9367,  4.6385, -0.6891, -1.6269,  9.2834,\n",
      "        -5.6118,  7.2611,  4.5844, -9.9984,  4.5780,  3.3895])\n",
      "\n",
      "Sample 3:\n",
      "Pruned Latent s0: tensor([ -4.2777,   0.5108,   3.6823,  -3.7846,   4.4797,  -0.9899,  -3.1272,\n",
      "         10.6987,  -4.1743,   5.1378,   4.0908, -11.3518,   4.3819,   4.4346])\n",
      "Action Description: You efficiently changed the state of the gleaming, automated, metal, tall, silver traffic light.\n",
      "Pruned Latent s1: tensor([-4.5459,  0.7781,  3.0355,  4.2527, -6.0900, -1.2778, -0.3453,  9.1166,\n",
      "        -4.0248,  6.1511,  3.3932, -7.2219,  6.7382,  4.3832])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from experiments.datasets import GridworldDataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer\n",
    "model_name = 'hf-hub:timm/ViT-B-16-SigLIP'\n",
    "tokenizer = get_tokenizer(model_name)\n",
    "\n",
    "\n",
    "# Initialize the tokenizer\n",
    "# model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def process_data(model, dataloader, prune_dims, device):\n",
    "    model.eval()\n",
    "    data_tuples = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Processing data\"):\n",
    "            # Extract data components\n",
    "            image_pairs, input_ids, token_type_ids, attention_mask = batch[0], batch[-4], batch[-3], batch[-2]\n",
    "\n",
    "            # Decode text descriptions\n",
    "            action_descriptions = [\n",
    "                tokenizer.tokenizer.decode(ids[mask.bool()].tolist(), skip_special_tokens=True)\n",
    "                for ids, mask in zip(input_ids, attention_mask)\n",
    "            ]\n",
    "\n",
    "            # Process each pair of images\n",
    "            for images, action_description in zip(image_pairs, action_descriptions):\n",
    "                # Encode images to latents and then to disentangled latents\n",
    "                s0, s1 = images[0].to(device), images[1].to(device)\n",
    "                latent_s0 = model.autoencoder.encoder(s0.unsqueeze(0))\n",
    "                latent_s1 = model.autoencoder.encoder(s1.unsqueeze(0))\n",
    "                disentangled_s0 = model.encode(latent_s0).squeeze(0)\n",
    "                disentangled_s1 = model.encode(latent_s1).squeeze(0)\n",
    "\n",
    "                # Prune specific latent dimensions using the prune_dims list\n",
    "                pruned_s0 = torch.index_select(disentangled_s0, 0, torch.tensor(prune_dims, device=device))\n",
    "                pruned_s1 = torch.index_select(disentangled_s1, 0, torch.tensor(prune_dims, device=device))\n",
    "\n",
    "                # Store the tuple (pruned_s0, action_description, pruned_s1)\n",
    "                data_tuples.append((pruned_s0.cpu(), action_description, pruned_s1.cpu()))\n",
    "\n",
    "    return data_tuples\n",
    "\n",
    "# Parameters\n",
    "data_folder = '/home/john/PhD/BISCUIT/data/gridworld_simplified_5c'\n",
    "batch_size = 32  # Adjusted for processing two images per sample\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Setup dataset and dataloader\n",
    "train_dataset = GridworldDataset(data_folder=data_folder, split='train', return_targets=True,\n",
    "                                 return_latents=True, single_image=False, triplet=False, seq_len=2,\n",
    "                                 cluster=False, return_text=True, subsample_percentage=0.005)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Process data\n",
    "model.to(device)\n",
    "data_tuples = process_data(model, train_loader, list(set(range(20)) - set([12, 13, 14, 15, 16, 18])), device)\n",
    "\n",
    "# Display some samples\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"Pruned Latent s0:\", data_tuples[i][0])\n",
    "    print(\"Action Description:\", data_tuples[i][1])\n",
    "    print(\"Pruned Latent s1:\", data_tuples[i][2])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_tuples(data_tuples, num_samples):\n",
    "    sequence_strings = []\n",
    "    for i, (s0, desc, s1) in enumerate(data_tuples[:num_samples]):\n",
    "        # Format tensors with two decimal places\n",
    "        s0_formatted = ', '.join(f\"{x:.2f}\" for x in s0.numpy())\n",
    "        s1_formatted = ', '.join(f\"{x:.2f}\" for x in s1.numpy())\n",
    "\n",
    "        # Create the sequence string for the current tuple\n",
    "        sequence_string = f\"Seq{i+1}: <[{s0_formatted}], {desc}, [{s1_formatted}]>\"\n",
    "        sequence_strings.append(sequence_string)\n",
    "\n",
    "    # Combine all sequence strings into one\n",
    "    full_sequence = ' '.join(sequence_strings)\n",
    "    return full_sequence\n",
    "\n",
    "# Define the number of sequences you want to generate in the string\n",
    "num_sequences = 30  # Change this number based on your requirements\n",
    "\n",
    "# Generate formatted sequence string\n",
    "formatted_sequences = format_data_tuples(data_tuples, num_sequences)\n",
    "print(formatted_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def create_json_from_tuples(data_tuples, num_samples):\n",
    "    sequences = {}\n",
    "    for i, (s0, desc, s1) in enumerate(data_tuples[:num_samples], start=1):\n",
    "        # Convert tensors to formatted lists\n",
    "        s0_list = [f\"{x:.2f}\" for x in s0.numpy()]\n",
    "        s1_list = [f\"{x:.2f}\" for x in s1.numpy()]\n",
    "\n",
    "        # Create a dictionary for the current sequence\n",
    "        sequence_dict = {\n",
    "            \"s_0\": s0_list,\n",
    "            \"a\": desc,\n",
    "            \"s_1\": s1_list\n",
    "        }\n",
    "\n",
    "        # Add the dictionary to the sequences with a key indicating the sequence number\n",
    "        sequences[f\"seq{i}\"] = sequence_dict\n",
    "\n",
    "    # Convert the dictionary to JSON\n",
    "    json_output = json.dumps(sequences, indent=4)\n",
    "    return json_output\n",
    "\n",
    "# Define the number of sequences you want to convert into JSON\n",
    "num_sequences = 300  # Adjust this to include more or fewer sequences\n",
    "\n",
    "# Create JSON from the data tuples\n",
    "json_sequences = create_json_from_tuples(data_tuples, num_sequences)\n",
    "# print(json_sequences)\n",
    "# Save json\n",
    "with open('pruned_sequences.json', 'w') as f:\n",
    "    f.write(json_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from data_generation.gridworld import Gridworld\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def format_causal_dict(causal_dict):\n",
    "    formatted_text = \"\"\n",
    "    for key, value in causal_dict.items():\n",
    "        formatted_text += f\"{key}: {value}\\n\"\n",
    "        if \"position_y\" in key:  # Add a line break after each entity's details\n",
    "            formatted_text += \"\\n\"\n",
    "    return formatted_text\n",
    "\n",
    "def visualize_episode(frames, causals, causal_keys, actions, action_descriptions, interventions):\n",
    "    for i in range(len(frames)):\n",
    "        # debug_causals = Gridworld.causal_vector_to_debug_dict(causal_keys, causals[i])\n",
    "        debug_causals = dict(zip(causal_keys, causals[i]))\n",
    "        formatted_causals = format_causal_dict(debug_causals)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax1 = plt.subplot(1, 2, 1)  # Frame subplot\n",
    "        ax1.imshow(frames[i])\n",
    "        ax1.set_title(f\"Frame {i+1}\")\n",
    "        ax1.axis('off')\n",
    "\n",
    "        ax2 = plt.subplot(1, 2, 2)  # Textual information subplot\n",
    "        ax2.axis('off')\n",
    "        text_info = (\n",
    "            f\"Step {i+1}:\\n\\n\"\n",
    "            f\"Causals (Formatted):\\n{formatted_causals}\\n\"\n",
    "            f\"Action: {actions[i]}\\n\"\n",
    "            f\"Action Description: {action_descriptions[i]}\\n\"\n",
    "            f\"Interventions: {interventions[i]}\"\n",
    "        )\n",
    "        ax2.text(0, 1, text_info, ha='left', va='top', fontsize=8, wrap=True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "a = np.load('/home/john/PhD/BISCUIT/data_generation/data/gridworld_simplified_2c2b2l_noturn_noshufflecars/check/gridworld_episode_28.npz')\n",
    "causal_keys = json.load(open('/home/john/PhD/BISCUIT/data_generation/data/gridworld_simplified_2c2b2l_noturn_noshufflecars/check_metadata.json'))['flattened_causals']\n",
    "frames, causals, actions, interventions, action_descriptions = a['frames'], a['causals'], a['actions'], a['interventions'], a['action_descriptions']\n",
    "visualize_episode(frames, causals, causal_keys, actions, action_descriptions, interventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "# Assume Gridworld, format_causal_dict, and the dataset class are defined elsewhere\n",
    "\n",
    "def visualize_episodes(dataset, causal_keys, N=1):\n",
    "    for episode_idx in range(min(N, len(dataset))):  # Loop through N episodes or the total dataset length\n",
    "        frame_seq = dataset[episode_idx]  # Get the episode data\n",
    "        frames, actions, interventions, causals = frame_seq\n",
    "        \n",
    "        for i in range(frames.shape[0] - 1):  # Iterate through each step, excluding the last frame\n",
    "            # Map causals to keys for the current frame\n",
    "            debug_causals = dict(zip(causal_keys, causals[i]))\n",
    "            formatted_causals = format_causal_dict(debug_causals)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax1 = plt.subplot(1, 2, 1)  # Frame subplot\n",
    "            # Adjust image normalization if necessary\n",
    "            img = (frames[i].permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0  # Normalize if required\n",
    "            ax1.imshow(img)\n",
    "            ax1.set_title(f\"Episode {episode_idx + 1}, Frame {i + 1}\")\n",
    "            ax1.axis('off')\n",
    "\n",
    "            ax2 = plt.subplot(1, 2, 2)  # Textual information subplot\n",
    "            ax2.axis('off')\n",
    "            text_info = (\n",
    "                f\"Episode {episode_idx + 1}, Step {i + 1}:\\n\\n\"\n",
    "                f\"Causals (Formatted):\\n{formatted_causals}\\n\"\n",
    "                f\"Action: {actions[i]}\\n\"\n",
    "                f\"Interventions: {interventions[i]}\"\n",
    "            )\n",
    "            ax2.text(0, 1, text_info, ha='left', va='top', fontsize=8, wrap=True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Example usage:\n",
    "causal_keys = json.load(open('/home/gkounto/BISCUIT/data_generation/data/gridworld_simplified/train_metadata.json'))['flattened_causals']\n",
    "# Assuming train_seq_dataset is defined and loaded\n",
    "visualize_episodes(train_seq_dataset, causal_keys, N=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_img(imgs: Any,\n",
    "             figure_title: str = None,\n",
    "             titles: Optional[list] = None):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axes = plt.subplots(1, len(imgs), figsize=(len(imgs) * 3.5, 3.5))\n",
    "    if len(imgs) == 1:\n",
    "        axes = [axes]\n",
    "    for i, ax in enumerate(axes):\n",
    "        if len(imgs[i].shape) == 3 and imgs[i].shape[0] in [3,4]:\n",
    "            imgs[i] = imgs[i].permute(1, 2, 0)\n",
    "        if isinstance(imgs[i], torch.Tensor):\n",
    "            imgs[i] = imgs[i].detach().cpu().numpy()\n",
    "        ax.imshow(imgs[i])\n",
    "        ax.axis('off')\n",
    "        if titles is not None and i < len(titles):\n",
    "            ax.set_title(titles[i], weight='bold')\n",
    "    if figure_title is not None:\n",
    "        fig.suptitle(figure_title, weight='bold', size=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_exclamation_mark_image(size=256, background_color=(1, 1, 1), mark_color=(1, 0, 0), mark_thickness=10, mark_height=100, dot_radius=10):\n",
    "    \"\"\"\n",
    "    Creates an image with a red exclamation mark in the center on a white background.\n",
    "\n",
    "    Parameters:\n",
    "    - size (int): The size of the image (width and height). Default is 256.\n",
    "    - background_color (tuple): The RGB color of the background in the range 0 to 1. Default is white.\n",
    "    - mark_color (tuple): The RGB color of the exclamation mark in the range 0 to 1. Default is red.\n",
    "    - mark_thickness (int): The thickness of the line part of the exclamation mark. Default is 10.\n",
    "    - mark_height (int): The height of the line part of the exclamation mark. Default is 100.\n",
    "    - dot_radius (int): The radius of the dot part of the exclamation mark. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The generated image as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Create a size x size x 3 array with the background color\n",
    "    image = np.ones((size, size, 3)) * np.array(background_color)\n",
    "\n",
    "    # Define the center position\n",
    "    center_x, center_y = size // 2, size // 2\n",
    "\n",
    "    # Draw the line part of the exclamation mark\n",
    "    for x in range(center_x - mark_thickness // 2, center_x + mark_thickness // 2):\n",
    "        for y in range(center_y - mark_height // 2, center_y + mark_height // 2 - dot_radius * 2):\n",
    "            image[y, x] = mark_color\n",
    "\n",
    "    # Draw the dot part of the exclamation mark\n",
    "    for x in range(image.shape[1]):\n",
    "        for y in range(image.shape[0]):\n",
    "            if (x - center_x) ** 2 + (y - (center_y + mark_height // 2 + dot_radius)) ** 2 <= dot_radius ** 2:\n",
    "                image[y, x] = mark_color\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def next_step_prediction(\n",
    "        model: BISCUITNF,\n",
    "        image: torch.Tensor,\n",
    "        action: torch.Tensor,\n",
    "        gt_image: torch.Tensor = torch.zeros(1),\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        plot_images: bool = True,\n",
    "        intv_targets: Optional[torch.Tensor] = None,\n",
    "        text: Optional[str] = None,\n",
    "        tokenizer: Optional[Any] = None,\n",
    "        text_only: bool = False,\n",
    "        N: int = 8\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if latents is None:\n",
    "        input_image = (image * 2.0) - 1.0\n",
    "        latents = model.autoencoder.encoder(input_image[None])\n",
    "        latents, _ = model.flow.forward(latents)\n",
    "    if text is not None:\n",
    "        tokenized_description = tokenizer.tokenizer(text, return_token_type_ids=True, padding='max_length', max_length=64)\n",
    "        input_ids = torch.tensor(tokenized_description['input_ids']).to(device)\n",
    "        token_type_ids = torch.tensor(tokenized_description['token_type_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokenized_description['attention_mask']).to(device)\n",
    "        tokenized_description = {'input_ids': input_ids, 'token_type_ids': token_type_ids, 'attention_mask': attention_mask}    \n",
    "    new_latents, _ = model.prior_t1.sample(latents, action[None], num_samples=1, intv_targets=intv_targets, tokenized_description=tokenized_description)\n",
    "    new_latents = new_latents.squeeze(1)\n",
    "    new_encodings = model.flow.reverse(new_latents)\n",
    "    new_image = model.autoencoder.decoder(new_encodings)[0]\n",
    "    new_image = (new_image + 1.0) / 2.0\n",
    "    if plot_images:\n",
    "        gt_diff_flag = False\n",
    "        new_image_frame = new_image.permute(1, 2, 0).cpu().numpy()\n",
    "        if latents is None:\n",
    "            old_image_frame = (image.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0\n",
    "        else:\n",
    "            old_image_frame = image.permute(1, 2, 0).cpu().numpy()\n",
    "        clicked_image_frame = np.copy(old_image_frame)\n",
    "        ground_truth_image_frame = (gt_image.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0\n",
    "        difference = np.abs(new_image_frame - ground_truth_image_frame)\n",
    "        print(f\"Mean absolute difference between the new image and the ground truth: {difference.mean()}\")\n",
    "        if difference.mean() > 0.001:\n",
    "            print(\"The new image is not close to the ground truth\")\n",
    "            gt_diff_flag = True\n",
    "            exclamation_mark_image = create_exclamation_mark_image()\n",
    "        if action.ndim == 1:\n",
    "            action = action[None]\n",
    "        for i in range(action.shape[0]):\n",
    "            if torch.any(action < 0) or torch.any(action > 1):\n",
    "                continue\n",
    "            # Correct calculation for pixel positions considering the normalized action coordinates\n",
    "            pixel_x = int(action[i, 0].item() * (image.shape[-1] - 1))\n",
    "            pixel_y = int(action[i, 1].item() * (image.shape[-2] - 1))\n",
    "            # Highlight the click location with a red color\n",
    "            clicked_image_frame[max(0, pixel_y-5):pixel_y+6, \n",
    "                                max(0, pixel_x-5):pixel_x+6, \n",
    "                                :] = np.array([1.0, 1.0, 1.0])\n",
    "        if torch.any(gt_image != 0):\n",
    "            image_list = [old_image_frame, clicked_image_frame, new_image_frame, ground_truth_image_frame]\n",
    "            image_titles = ['Previous Frame', 'Click Location', 'New Sample', 'Ground Truth']\n",
    "            if gt_diff_flag:\n",
    "                image_list.append(exclamation_mark_image)\n",
    "                image_titles.append('Difference')\n",
    "            show_img(image_list,\n",
    "                     figure_title=f'Performing action {(action if action.ndim == 1 else action[0]).squeeze().cpu().numpy()}',\n",
    "                     titles=image_titles)\n",
    "        else:\n",
    "            show_img([old_image_frame, clicked_image_frame, new_image_frame],\n",
    "                    figure_title=f'Performing action {(action if action.ndim == 1 else action[0]).squeeze().cpu().numpy()}',\n",
    "                    titles=['Previous Frame', 'Click Location', 'New Sample'])\n",
    "    return new_image, new_latents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    }
   ],
   "source": [
    "val_dataset = GridworldDataset(\n",
    "\tdata_folder=data_folder, split='val', return_targets=False, single_image=True, return_latents=True, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=1.0,)\n",
    "for elem in val_dataset:\n",
    "    print(elem[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.datasets import GridworldDataset\n",
    "data_folder = '/home/john/PhD/BISCUIT/data/gridworld_simplified_5c'\n",
    "# val_seq_dataset = GridworldDataset(\n",
    "# data_folder=data_folder, split='val', return_targets=True, return_robot_state=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=0.01)\n",
    "test_seq_dataset = GridworldDataset(\n",
    "        data_folder=data_folder, split='test', return_targets=True, return_latents=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=0.01)\n",
    "val_indep_dataset = GridworldDataset(\n",
    "         data_folder=data_folder, split='val_indep', return_targets=True, single_image=True, return_latents=True, triplet=False, seq_len=1, cluster=False, return_text=False, subsample_percentage=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def reverseEnum(data: list):\n",
    "    for i in range(len(data)-1, -1, -1):\n",
    "        yield (i, data[i])\n",
    "\n",
    "for i, frame_seq in enumerate(test_seq_dataset):\n",
    "        print(i)\n",
    "        action = torch.tensor(frame_seq[1]).to(device).squeeze()\n",
    "        interventions = torch.tensor(frame_seq[2]).to(device)\n",
    "        causals = torch.tensor(frame_seq[3]).to(device)\n",
    "        print(causals)\n",
    "        print(interventions)\n",
    "        # intv_targets = torch.tensor(frame_seq[2]).to(device)\n",
    "        intv_targets = None\n",
    "        plot_images = True\n",
    "        new_image, new_latents = next_step_prediction(model, frame_seq[0][0], action, gt_image=frame_seq[0][1], plot_images=plot_images, intv_targets=intv_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, frame_seq in enumerate(val_seq_dataset):\n",
    "    # Stack the images at each step to form a sequence\n",
    "    frames, actions = frame_seq\n",
    "    frames = frames.to(device)\n",
    "    actions = torch.from_numpy(actions).to(device)\n",
    "    # interventions = interventions.to(device)\n",
    "    # causals = causals.to(device)\n",
    "    # Perform the next step prediction\n",
    "    new_image, new_latents = next_step_prediction(frames[0], actions, gt_image=frames[1], intv_targets=None, plot_images=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "latents = None\n",
    "image = test_seq_dataset[0][0][0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "load_text = ax.text(image.shape[-1]//2, \n",
    "                image.shape[-2]//2, \n",
    "                'Loading...', \n",
    "                fontsize='x-large',\n",
    "                weight='bold',\n",
    "                va='center',\n",
    "                ha='center',\n",
    "                backgroundcolor=(1.0, 0.8, 0.8))\n",
    "load_text.set_visible(False)\n",
    "ax.axis('off')\n",
    "\n",
    "def onclick(event):\n",
    "    print('click')\n",
    "    global image, latents\n",
    "    load_text.set_visible(True)\n",
    "    fig.canvas.draw()\n",
    "    ix, iy = event.xdata, event.ydata\n",
    "    # ix = (ix / image.shape[-1] - 0.5) * 2.0\n",
    "    # iy = (iy / image.shape[-2] - 0.5) * 2.0\n",
    "    print(f'Clicked at x={ix}, y={iy}')\n",
    "    image, latents = next_step_prediction(model, image=image,\n",
    "                                            action=torch.tensor([iy, ix], \n",
    "                                                                    dtype=torch.float32,\n",
    "                                                                    device=device),\n",
    "                                            latents=latents,\n",
    "                                            plot_images=False)\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    load_text.set_visible(False)\n",
    "    fig.canvas.draw()\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = test_seq_dataset[5][0][0]\n",
    "latents = None\n",
    "action_sequence = [\n",
    "    (0.42857143, 1.),\n",
    "    (0., 0.),\n",
    "    (-0.14285714, -0.14285714),\n",
    "    (0.42857143, 1.),\n",
    "    (0., 0.),\n",
    "]\n",
    "\n",
    "for i, action in enumerate(action_sequence):\n",
    "    image, latents = next_step_prediction(model,\n",
    "                                          image=image, \n",
    "                                          action=torch.tensor(action, device=device), \n",
    "                                          latents=latents,\n",
    "                                          gt_image=test_seq_dataset[5][0][1],\n",
    "                                          plot_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "# Assume Gridworld, format_causal_dict, and the dataset class are defined elsewhere\n",
    "\n",
    "def visualize_episodes(dataset, causal_keys, N=1):\n",
    "    for episode_idx in range(min(N, len(dataset))):  # Loop through N episodes or the total dataset length\n",
    "        frame_seq = dataset[episode_idx]  # Get the episode data\n",
    "        frames, actions, interventions, causals = frame_seq\n",
    "        \n",
    "        for i in range(frames.shape[0] - 1):  # Iterate through each step, excluding the last frame\n",
    "            # Map causals to keys for the current frame\n",
    "            debug_causals = dict(zip(causal_keys, causals[i]))\n",
    "            print(debug_causals)\n",
    "            formatted_causals = format_causal_dict(debug_causals)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax1 = plt.subplot(1, 2, 1)  # Frame subplot\n",
    "            # Adjust image normalization if necessary\n",
    "            img = (frames[i].permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0  # Normalize if required\n",
    "            ax1.imshow(img)\n",
    "            ax1.set_title(f\"Episode {episode_idx + 1}, Frame {i + 1}\")\n",
    "            ax1.axis('off')\n",
    "\n",
    "            ax2 = plt.subplot(1, 2, 2)  # Textual information subplot\n",
    "            ax2.axis('off')\n",
    "            text_info = (\n",
    "                f\"Episode {episode_idx + 1}, Step {i + 1}:\\n\\n\"\n",
    "                f\"Causals (Formatted):\\n{formatted_causals}\\n\"\n",
    "                f\"Action: {actions[i]}\\n\"\n",
    "                f\"Interventions: {interventions[i]}\"\n",
    "            )\n",
    "            ax2.text(0, 1, text_info, ha='left', va='top', fontsize=8, wrap=True)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Example usage:\n",
    "causal_keys = json.load(open('/home/john/PhD/BISCUIT/data/gridworld_simplified_5c/train_metadata.json'))['flattened_causals']\n",
    "# Assuming train_seq_dataset is defined and loaded\n",
    "visualize_episodes(train_seq_dataset, causal_keys, N=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_img(imgs: Any,\n",
    "             figure_title: str = None,\n",
    "             titles: Optional[list] = None):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axes = plt.subplots(1, len(imgs), figsize=(len(imgs) * 3.5, 3.5))\n",
    "    if len(imgs) == 1:\n",
    "        axes = [axes]\n",
    "    for i, ax in enumerate(axes):\n",
    "        if len(imgs[i].shape) == 3 and imgs[i].shape[0] in [3,4]:\n",
    "            imgs[i] = imgs[i].permute(1, 2, 0)\n",
    "        if isinstance(imgs[i], torch.Tensor):\n",
    "            imgs[i] = imgs[i].detach().cpu().numpy()\n",
    "        ax.imshow(imgs[i])\n",
    "        ax.axis('off')\n",
    "        if titles is not None and i < len(titles):\n",
    "            ax.set_title(titles[i], weight='bold')\n",
    "    if figure_title is not None:\n",
    "        fig.suptitle(figure_title, weight='bold', size=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_exclamation_mark_image(size=256, background_color=(1, 1, 1), mark_color=(1, 0, 0), mark_thickness=10, mark_height=100, dot_radius=10):\n",
    "    \"\"\"\n",
    "    Creates an image with a red exclamation mark in the center on a white background.\n",
    "\n",
    "    Parameters:\n",
    "    - size (int): The size of the image (width and height). Default is 256.\n",
    "    - background_color (tuple): The RGB color of the background in the range 0 to 1. Default is white.\n",
    "    - mark_color (tuple): The RGB color of the exclamation mark in the range 0 to 1. Default is red.\n",
    "    - mark_thickness (int): The thickness of the line part of the exclamation mark. Default is 10.\n",
    "    - mark_height (int): The height of the line part of the exclamation mark. Default is 100.\n",
    "    - dot_radius (int): The radius of the dot part of the exclamation mark. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The generated image as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Create a size x size x 3 array with the background color\n",
    "    image = np.ones((size, size, 3)) * np.array(background_color)\n",
    "\n",
    "    # Define the center position\n",
    "    center_x, center_y = size // 2, size // 2\n",
    "\n",
    "    # Draw the line part of the exclamation mark\n",
    "    for x in range(center_x - mark_thickness // 2, center_x + mark_thickness // 2):\n",
    "        for y in range(center_y - mark_height // 2, center_y + mark_height // 2 - dot_radius * 2):\n",
    "            image[y, x] = mark_color\n",
    "\n",
    "    # Draw the dot part of the exclamation mark\n",
    "    for x in range(image.shape[1]):\n",
    "        for y in range(image.shape[0]):\n",
    "            if (x - center_x) ** 2 + (y - (center_y + mark_height // 2 + dot_radius)) ** 2 <= dot_radius ** 2:\n",
    "                image[y, x] = mark_color\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def next_step_prediction(\n",
    "        model: BISCUITNF,\n",
    "        image: torch.Tensor,\n",
    "        action: torch.Tensor,\n",
    "        gt_image: torch.Tensor = torch.zeros(1),\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        plot_images: bool = True,\n",
    "        intv_targets: Optional[torch.Tensor] = None,\n",
    "        text: Optional[str] = None,\n",
    "        tokenizer: Optional[Any] = None,\n",
    "        text_only: bool = False,\n",
    "        tokenized_description=None,\n",
    "        N: int = 8\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if latents is None:\n",
    "        input_image = (image * 2.0) - 1.0\n",
    "        latents = model.autoencoder.encoder(input_image[None])\n",
    "        latents, _ = model.flow.forward(latents)\n",
    "    # tokenized_description = None\n",
    "    if text is not None:\n",
    "        tokenized_description = tokenizer(text, return_token_type_ids=True, padding='max_length', max_length=64)\n",
    "        input_ids = torch.tensor(tokenized_description['input_ids']).to(device)\n",
    "        token_type_ids = torch.tensor(tokenized_description['token_type_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokenized_description['attention_mask']).to(device)\n",
    "        tokenized_description = {'input_ids': input_ids, 'token_type_ids': token_type_ids, 'attention_mask': attention_mask}    \n",
    "    new_latents, _ = model.prior_t1.sample(latents, action[None], num_samples=1, intv_targets=intv_targets, tokenized_description=tokenized_description)\n",
    "    new_latents = new_latents.squeeze(1)\n",
    "    new_encodings = model.flow.reverse(new_latents)\n",
    "    new_image = model.autoencoder.decoder(new_encodings)[0]\n",
    "    new_image = (new_image + 1.0) / 2.0\n",
    "    if plot_images:\n",
    "        gt_diff_flag = False\n",
    "        new_image_frame = new_image.permute(1, 2, 0).cpu().numpy()\n",
    "        if latents is None:\n",
    "            old_image_frame = (image.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0\n",
    "        else:\n",
    "            old_image_frame = image.permute(1, 2, 0).cpu().numpy()\n",
    "        clicked_image_frame = np.copy(old_image_frame)\n",
    "        ground_truth_image_frame = (gt_image.permute(1, 2, 0).cpu().numpy() + 1.0) / 2.0\n",
    "        difference = np.abs(new_image_frame - ground_truth_image_frame)\n",
    "        print(f\"Mean absolute difference between the new image and the ground truth: {difference.mean()}\")\n",
    "        if difference.mean() > 0.001:\n",
    "            print(\"The new image is not close to the ground truth\")\n",
    "            gt_diff_flag = True\n",
    "            exclamation_mark_image = create_exclamation_mark_image()\n",
    "        if action.ndim == 1:\n",
    "            action = action[None]\n",
    "        for i in range(action.shape[0]):\n",
    "            if torch.any(action < 0) or torch.any(action > 1):\n",
    "                continue\n",
    "            # Correct calculation for pixel positions considering the normalized action coordinates\n",
    "            pixel_x = int(action[i, 0].item() * (image.shape[-1] - 1))\n",
    "            pixel_y = int(action[i, 1].item() * (image.shape[-2] - 1))\n",
    "            # Highlight the click location with a red color\n",
    "            clicked_image_frame[max(0, pixel_y-5):pixel_y+6, \n",
    "                                max(0, pixel_x-5):pixel_x+6, \n",
    "                                :] = np.array([1.0, 1.0, 1.0])\n",
    "        if torch.any(gt_image != 0):\n",
    "            image_list = [old_image_frame, clicked_image_frame, new_image_frame, ground_truth_image_frame]\n",
    "            image_titles = ['Previous Frame', 'Click Location', 'New Sample', 'Ground Truth']\n",
    "            if gt_diff_flag:\n",
    "                image_list.append(exclamation_mark_image)\n",
    "                image_titles.append('Difference')\n",
    "            show_img(image_list,\n",
    "                     figure_title=f'Performing action {(action if action.ndim == 1 else action[0]).squeeze().cpu().numpy()}',\n",
    "                     titles=image_titles)\n",
    "        else:\n",
    "            show_img([old_image_frame, clicked_image_frame, new_image_frame],\n",
    "                    figure_title=f'Performing action {(action if action.ndim == 1 else action[0]).squeeze().cpu().numpy()}',\n",
    "                    titles=['Previous Frame', 'Click Location', 'New Sample'])\n",
    "    return new_image, new_latents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = GridworldDataset(\n",
    "\tdata_folder=data_folder, split='val_indep', return_targets=False, single_image=True, return_latents=True, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=1.0,)\n",
    "for elem in val_dataset:\n",
    "    print(elem[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sequences of val:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    }
   ],
   "source": [
    "from experiments.datasets import GridworldDataset\n",
    "data_folder = '/home/john/PhD/BISCUIT/data_generation/data/gridworld_simplified_3c1b3l_noturn_noshufflecars_f'\n",
    "# val_seq_dataset = GridworldDataset(\n",
    "# data_folder=data_folder, split='val', return_targets=True, return_robot_state=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=0.01)\n",
    "test_seq_dataset = GridworldDataset(\n",
    "        data_folder=data_folder, split='val', return_targets=True, return_latents=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=True, subsample_percentage=0.01)\n",
    "# val_indep_dataset = GridworldDataset(\n",
    "#          data_folder=data_folder, split='val_indep', return_targets=True, single_image=True, return_latents=True, triplet=False, seq_len=1, cluster=False, return_text=True, subsample_percentage=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "test_elem = test_seq_dataset[0]\n",
    "input_ids, token_type_ids, attention_mask = test_elem[-4:-1]\n",
    "tokenizer.decode(input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def reverseEnum(data: list):\n",
    "    for i in range(len(data)-1, -1, -1):\n",
    "        yield (i, data[i])\n",
    "\n",
    "for i, frame_seq in enumerate(test_seq_dataset):\n",
    "        print(i)\n",
    "        action = torch.tensor(frame_seq[1]).to(device).squeeze()\n",
    "        interventions = torch.tensor(frame_seq[2]).to(device)\n",
    "        causals = torch.tensor(frame_seq[3]).to(device)\n",
    "        tokenized_description = {'input_ids': torch.tensor(frame_seq[3]).to(device), 'token_type_ids': torch.tensor(frame_seq[4]).to(device), 'attention_mask': torch.tensor(frame_seq[5]).to(device)}\n",
    "        print(causals)\n",
    "        print(interventions)\n",
    "        # intv_targets = torch.tensor(frame_seq[2]).to(device)\n",
    "        intv_targets = None\n",
    "        plot_images = True\n",
    "        new_image, new_latents = next_step_prediction(model, frame_seq[0][0], action, gt_image=frame_seq[0][1], plot_images=plot_images, intv_targets=intv_targets, tokenized_description=tokenized_description, text_only=False, N=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq_dataset[94][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "frame1 = test_seq_dataset[idx][0][0]\n",
    "frame2 = test_seq_dataset[idx][0][1]\n",
    "tokenized_description = {'input_ids': torch.tensor(test_seq_dataset[idx][3]).to(device), 'token_type_ids': torch.tensor(test_seq_dataset[idx][4]).to(device), 'attention_mask': torch.tensor(test_seq_dataset[idx][5]).to(device)}\n",
    "zdt = model.flow(model.autoencoder.encoder(frame1[None]))\n",
    "zdtplus1 = model.flow(model.autoencoder.encoder(frame2[None]))\n",
    "zdtplus1prior = model.prior_t1.sample(zdt[0], action=torch.tensor(test_seq_dataset[94][1]).to(device), num_samples=1, intv_targets=None, tokenized_description=tokenized_description)\n",
    "print(zdt)\n",
    "print(zdtplus1)\n",
    "zdtplus1flow = model.flow.reverse(zdtplus1[0])\n",
    "zdtplus1flow = model.autoencoder.decoder(zdtplus1flow)\n",
    "zdtplus1flow = (zdtplus1flow + 1) / 2\n",
    "zdtplus1prior_ = model.flow.reverse(zdtplus1prior[0])\n",
    "zdtplus1prior_ = model.autoencoder.decoder(zdtplus1prior_)\n",
    "zdtplus1prior_ = (zdtplus1prior_ + 1) / 2\n",
    "from matplotlib import pyplot as plt\n",
    "# plt.imshow(zt.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "# plt.imshow(zdtplus1prior_.squeeze().cpu().detach().numpy().transpose(1, 2, 0))\n",
    "plt.imshow(zdtplus1flow.squeeze().cpu().detach().numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zdtplus1prior[0] - zdtplus1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, frame_seq in enumerate(val_seq_dataset):\n",
    "    # Stack the images at each step to form a sequence\n",
    "    frames, actions = frame_seq\n",
    "    frames = frames.to(device)\n",
    "    actions = torch.from_numpy(actions).to(device)\n",
    "    # interventions = interventions.to(device)\n",
    "    # causals = causals.to(device)\n",
    "    # Perform the next step prediction\n",
    "    new_image, new_latents = next_step_prediction(frames[0], actions, gt_image=frames[1], intv_targets=None, plot_images=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "latents = None\n",
    "image = test_seq_dataset[0][0][0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "load_text = ax.text(image.shape[-1]//2, \n",
    "                image.shape[-2]//2, \n",
    "                'Loading...', \n",
    "                fontsize='x-large',\n",
    "                weight='bold',\n",
    "                va='center',\n",
    "                ha='center',\n",
    "                backgroundcolor=(1.0, 0.8, 0.8))\n",
    "load_text.set_visible(False)\n",
    "ax.axis('off')\n",
    "\n",
    "def onclick(event):\n",
    "    print('click')\n",
    "    global image, latents\n",
    "    load_text.set_visible(True)\n",
    "    fig.canvas.draw()\n",
    "    ix, iy = event.xdata, event.ydata\n",
    "    # ix = (ix / image.shape[-1] - 0.5) * 2.0\n",
    "    # iy = (iy / image.shape[-2] - 0.5) * 2.0\n",
    "    print(f'Clicked at x={ix}, y={iy}')\n",
    "    image, latents = next_step_prediction(model, image=image,\n",
    "                                            action=torch.tensor([iy, ix], \n",
    "                                                                    dtype=torch.float32,\n",
    "                                                                    device=device),\n",
    "                                            latents=latents,\n",
    "                                            plot_images=False)\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    load_text.set_visible(False)\n",
    "    fig.canvas.draw()\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = test_seq_dataset[5][0][0]\n",
    "latents = None\n",
    "action_sequence = [\n",
    "    (0.42857143, 1.),\n",
    "    (0., 0.),\n",
    "    (-0.14285714, -0.14285714),\n",
    "    (0.42857143, 1.),\n",
    "    (0., 0.),\n",
    "]\n",
    "\n",
    "for i, action in enumerate(action_sequence):\n",
    "    image, latents = next_step_prediction(model,\n",
    "                                          image=image, \n",
    "                                          action=torch.tensor(action, device=device), \n",
    "                                          latents=latents,\n",
    "                                          gt_image=test_seq_dataset[5][0][1],\n",
    "                                          plot_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7304e1d3ab36474d82940075e638b87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Action:', placeholder='Type action here'), Button(description='Upda…"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4475303fbb4cb7817bbc8e27295f08",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=640.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You performed no action.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: you quickly move the orange obstacle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: you quickly move the orange obstacle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: you quickly move the orange obstacle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: you quickly move the orange obstacle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: you quickly move the orange obstacle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: you quickly move the orange obstacle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: you quickly move the orange obstacle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: you quickly move the orange obstacle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the blue vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the blue vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the blue vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the blue vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the blue vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the blue vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the blue vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the blue vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the blue vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You move the silver vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You changed the state of the cyan traffic light.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You performed no action.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You moved the red vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You moved the red vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You moved the red vehicle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You performed no action.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You performed no action.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: You performed no action.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib ipympl\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "# from open_clip import get_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "# tokenizer = get_tokenizer('hf-hub:timm/ViT-B-16-SigLIP')\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "action = torch.tensor((-0.14285714, -0.14285714)).to(device)\n",
    "\n",
    "# Assuming test_seq_dataset, next_step_prediction, and device are defined elsewhere\n",
    "latents = None\n",
    "image = test_seq_dataset[0][0][0]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "load_text = ax.text(image.shape[-1]//2, \n",
    "                    image.shape[-2]//2, \n",
    "                    'Enter action and press \"Update\"', \n",
    "                    fontsize='x-large',\n",
    "                    weight='bold',\n",
    "                    va='center',\n",
    "                    ha='center',\n",
    "                    backgroundcolor=(1.0, 0.8, 0.8))\n",
    "load_text.set_visible(True)  # Initially visible to prompt for input\n",
    "ax.axis('off')\n",
    "\n",
    "# Text input for action\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type action here',\n",
    "    description='Action:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Button to trigger the update\n",
    "update_button = widgets.Button(\n",
    "    description='Update',\n",
    "    disabled=False,\n",
    "    button_style='',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to update based on action',\n",
    "    icon='check'  # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "def on_update_button_clicked(b):\n",
    "    global image, latents\n",
    "    text = text_input.value \n",
    "    \n",
    "    \n",
    "    print(f'Action: {text}') \n",
    "\n",
    "    # Update the image and latents based on the provided action\n",
    "    image, latents = next_step_prediction(model,\n",
    "                                          image=image,\n",
    "                                          action=action,  # Assuming the function now accepts a string\n",
    "                                          latents=latents,\n",
    "                                          plot_images=False,\n",
    "                                          text=text,\n",
    "                                          tokenizer=tokenizer,\n",
    "                                          text_only=True)\n",
    "                                          \n",
    "    \n",
    "    # Update the plot with the new image\n",
    "    ax.clear()  # Clear the previous image\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    load_text.set_visible(False)  # Hide the loading text\n",
    "    ax.axis('off')  # Hide axes again after redrawing\n",
    "    fig.canvas.draw()  # Refresh the figure\n",
    "\n",
    "update_button.on_click(on_update_button_clicked)\n",
    "\n",
    "# Display the widgets\n",
    "widgets.VBox([text_input, update_button])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(map(get_color_name, [eval(x.split('_')[1]) for x in causal_dicts[0].keys()]))\n",
    "# Replace the rgb values with the color names in the causal_dicts keys while keeping the rest of the key intact\n",
    "# i.e., 'obstacle_(255, 165, 0)_position_x' -> 'obstacle_orange_position_x'\n",
    "plt.imshow(frame1, origin='upper', interpolation='none')\n",
    "# Remove the axes and the white space around the image\n",
    "plt.gca().set_axis_off()\n",
    "# Remove the white space around the image\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame2, origin='upper', interpolation='none')\n",
    "# Remove the axes and the white space around the image\n",
    "plt.gca().set_axis_off()\n",
    "# Remove the white space around the image\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from models.ae import Autoencoder\n",
    "\n",
    "import io\n",
    "import base64\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from dash import Dash, dcc, html, Input, Output, no_update, callback\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "import io\n",
    "import base64\n",
    "import dash\n",
    "\n",
    "# Helper functions\n",
    "def np_image_to_base64(img):\n",
    "    \"\"\"Convert a NumPy array to a base64 encoded image.\"\"\"\n",
    "    img = Image.fromarray((img * 255).astype(np.uint8))\n",
    "    buffered = io.BytesIO()\n",
    "    img.save(buffered, format=\"PNG\")\n",
    "    return f\"data:image/png;base64,{base64.b64encode(buffered.getvalue()).decode()}\"\n",
    "\n",
    "# Main function to prepare data and create Dash app\n",
    "def create_visualizations_app(model, dataloader, perplexity=30, n_neighbors=15, min_dist=0.1, max_samples=500):\n",
    "    model.eval()\n",
    "    embeddings, images, frame_positions = [], [], []\n",
    "    sample_count = 0\n",
    "\n",
    "    # Collect data\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            imgs, positions, *_ = batch  # Update this based on your data structure\n",
    "            if imgs.dim() > 4:\n",
    "                imgs = imgs.view(-1, *imgs.shape[2:])\n",
    "                positions = positions.view(-1)\n",
    "\n",
    "            if sample_count + imgs.shape[0] > max_samples:\n",
    "                limit = max_samples - sample_count\n",
    "                imgs = imgs[:limit]\n",
    "                positions = positions[:limit]\n",
    "\n",
    "            embeddings_batch = model(imgs.to('cuda')).cpu().numpy()  # Ensure device compatibility\n",
    "            embeddings.append(embeddings_batch)\n",
    "            images.extend([np_image_to_base64(np.array(img.permute(1, 2, 0))) for img in imgs])\n",
    "            frame_positions.extend(positions.cpu().numpy())\n",
    "\n",
    "            sample_count += imgs.shape[0]\n",
    "            if sample_count >= max_samples:\n",
    "                break\n",
    "\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    colors = frame_positions / np.max(frame_positions)  # Normalize for color mapping\n",
    "\n",
    "    # Perform dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity).fit_transform(embeddings)\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=2)\n",
    "    umap_result = reducer.fit_transform(embeddings)\n",
    "\n",
    "    # Create Dash app\n",
    "    app = Dash(__name__)\n",
    "\n",
    "    app.layout = html.Div([\n",
    "        dcc.Graph(id=\"graph-tsne\", figure=create_figure(tsne, images, frame_positions, 't-SNE Visualization')),\n",
    "        dcc.Graph(id=\"graph-umap\", figure=create_figure(umap_result, images, frame_positions, 'UMAP Visualization')),\n",
    "        dcc.Tooltip(id=\"graph-tooltip\")\n",
    "    ])\n",
    "\n",
    "    @callback(\n",
    "        Output(\"graph-tooltip\", \"show\"),\n",
    "        Output(\"graph-tooltip\", \"bbox\"),\n",
    "        Output(\"graph-tooltip\", \"children\"),\n",
    "        Input(\"graph-tsne\", \"hoverData\"),\n",
    "        Input(\"graph-umap\", \"hoverData\")\n",
    "    )\n",
    "    def display_hover(hoverData_tsne, hoverData_umap):\n",
    "        ctx = dash.callback_context\n",
    "\n",
    "        if not ctx.triggered:\n",
    "            return False, no_update, no_update\n",
    "\n",
    "        hover_data = ctx.triggered[0][\"value\"][\"points\"][0]\n",
    "        bbox = hover_data[\"bbox\"]\n",
    "        num = hover_data[\"pointNumber\"]\n",
    "\n",
    "        children = html.Div([\n",
    "            html.Img(src=images[num], style={\"width\": \"100px\", 'display': 'block', 'margin': '0 auto'}),\n",
    "            # html.P(f\"Frame Position: {frame_positions[num]}\", style={'font-weight': 'bold'})\n",
    "        ])\n",
    "\n",
    "        return True, bbox, children\n",
    "\n",
    "    return app\n",
    "\n",
    "def create_figure(data, images, frame_positions, title):\n",
    "    fig = go.Figure(data=[go.Scatter(\n",
    "        x=data[:, 0],\n",
    "        y=data[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(size=5, color=frame_positions, colorscale='Viridis', showscale=True),\n",
    "        text=[f'' for img, pos in zip(images, frame_positions)],\n",
    "        hoverinfo='text'\n",
    "    )])\n",
    "    fig.update_layout(title=title, xaxis_title=f'{title} Dimension 1', yaxis_title=f'{title} Dimension 2')\n",
    "    return fig\n",
    "\n",
    "model = Autoencoder.load_from_checkpoint('/home/john/PhD/BISCUIT/pretrained_models/epoch=14-step=8325.ckpt')\n",
    "from experiments.datasets import GridworldDataset\n",
    "data_folder = '/home/john/PhD/BISCUIT/data_generation/data/gridworld_simplified_2c2b2l_noturn_noshufflecars_bfix_preintv'\n",
    "train_seq_dataset = GridworldDataset(\n",
    "        data_folder=data_folder, split='val', return_targets=True, return_latents=True, single_image=False, triplet=False, seq_len=2, cluster=False, return_text=False, subsample_percentage=1.0, return_whole_episode=True)\n",
    "train_seq_dataloader = torch.utils.data.DataLoader(train_seq_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Run the app\n",
    "app = create_visualizations_app(model, train_seq_dataloader, perplexity=30, n_neighbors=100, min_dist=0.1, max_samples=300)\n",
    "app.run_server(debug=True, port=8050, host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 100.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_goals(example, return_raw=False):\n",
    "    \"\"\"Extract the goals from the example.\n",
    "    \n",
    "    :param example: dict with 'question' key containing goal and plan statements\n",
    "    :param return_raw: if True, returns the raw goal statement\n",
    "    :return: Either raw goal string or parsed goals as a dictionary\n",
    "    \"\"\"\n",
    "    goal_statement = example[\"question\"].split(\"[STATEMENT]\")[-1]\\\n",
    "        .split(\"My goal is to \")[1].split(\"My plan is as follows\")[0].strip()\n",
    "    \n",
    "    if return_raw:\n",
    "        return goal_statement\n",
    "    \n",
    "    # Improved regular expression to extract goal elements (entity, attribute, value)\n",
    "    pattern = r\"(\\w+)\\s(\\w+\\s\\w+)\\sis\\s([\\d\\.]+)(?=\\,|\\s|$)\"\n",
    "    goals = {match.group(1) + ' ' + match.group(2): float(match.group(3).rstrip('.')) for match in re.finditer(pattern, goal_statement)}\n",
    "    \n",
    "    return goals\n",
    "\n",
    "def goal_check(goals, description, epsilon=0.07, ignore_obstacles=False):\n",
    "    \"\"\"Check if the description matches the goals with a tolerance and return the percentage of goals met.\n",
    "    \n",
    "    :param goals: dictionary of goal states\n",
    "    :param description: description string containing current states\n",
    "    :param epsilon: tolerance for numeric comparisons\n",
    "    :param ignore_obstacles: if True, obstacle positions are ignored in the check\n",
    "    :return: Tuple (boolean, float) where boolean is True if all goals are met within tolerance, and float is the percentage of goals met\n",
    "    \"\"\"\n",
    "    # Parse the description into a dictionary\n",
    "    pattern = r\"(\\w+)\\s(\\w+\\s\\w+)\\sis\\s([\\d\\.]+)(?=\\,|\\s|$)\"\n",
    "    current_states = {match.group(1) + ' ' + match.group(2): float(match.group(3).rstrip('.')) for match in re.finditer(pattern, description)}\n",
    "    \n",
    "    total_goals = 0\n",
    "    met_goals = 0\n",
    "    for key, goal_value in goals.items():\n",
    "        if ignore_obstacles and 'obstacle' in key:\n",
    "            continue\n",
    "        total_goals += 1\n",
    "        if key in current_states and (abs(current_states[key] - goal_value) <= epsilon):\n",
    "            met_goals += 1\n",
    "\n",
    "    all_goals_met = met_goals == total_goals\n",
    "    percentage_met = (met_goals / total_goals) * 100 if total_goals > 0 else 0\n",
    "    return all_goals_met, percentage_met\n",
    "\n",
    "# Testing\n",
    "example = {\n",
    "    \"question\": \"[STATEMENT] This is the context. My goal is to orange obstacle position x is 1.0, orange obstacle position y is 0.86, cyan trafficlight state is 0.00, olive trafficlight state is 1.00, silver trafficlight state is 0.00, blue vehicle position y is 0.86, silver vehicle position y is 0.71, red vehicle position y is 0.14. My plan is as follows: execute movement.\"\n",
    "}\n",
    "description = \"orange obstacle position x is 0.57, orange obstacle position y is 0.86, cyan trafficlight state is 0.00, olive trafficlight state is 1.00, silver trafficlight state is 0.00, blue vehicle position y is 0.86, silver vehicle position y is 0.71, red vehicle position y is 0.14.\"\n",
    "\n",
    "goals = extract_goals(example)\n",
    "result = goal_check(goals, description, ignore_obstacles=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
